{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94771,"databundleVersionId":13613251,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13256549,"sourceType":"datasetVersion","datasetId":8400276}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":765.21753,"end_time":"2025-09-08T12:22:14.944658","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-08T12:09:29.727128","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bade4ff9","cell_type":"markdown","source":"---\n\n**In this notebook, we will use Ensembling Gradient Boosting trees for a multi-target regression problem, leveraging lagged targets to predict 424 outputs. To speed up inference, we adopt the long-format multi-output prediction method, which is much faster than the standard multi-output approach.**\n\n**You will also find several useful techniques in this notebook, including:**\n\n* How to create lagged targets and use them in the prediction step\n* How to run Ensembling Gradient Boosting trees with lags targets\n* How to build an optimized prediction function for API inference\n\n\n\n---","metadata":{"papermill":{"duration":0.00386,"end_time":"2025-09-08T12:09:35.266874","exception":false,"start_time":"2025-09-08T12:09:35.263014","status":"completed"},"tags":[]}},{"id":"c65a313c","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Data loading\np = '/kaggle/input/mitsui-commodity-prediction-challenge/'\ntrain = pd.read_csv(p+'train.csv')\ntrainl = pd.read_csv(p+'train_labels.csv')\ntraint = pd.read_csv(p+'target_pairs.csv')\n\ndef _handle_missing_values(data):\n    data.interpolate(method='polynomial', order=3, inplace=True)\n    data.clip(lower=-10, upper=10, inplace=True)\n    return data\n\ntrain = _handle_missing_values(train)\ntrainl = _handle_missing_values(trainl)\n\ntarget_lag_1 = traint.loc[traint[\"lag\"]==1,\"target\"].values\ntarget_lag_2 = traint.loc[traint[\"lag\"]==2,\"target\"].values\ntarget_lag_3 = traint.loc[traint[\"lag\"]==3,\"target\"].values\ntarget_lag_4 = traint.loc[traint[\"lag\"]==4,\"target\"].values\n\nFeatures = [i for i in trainl.columns]\n\ndef create_lagged_labels(df):\n    dt = pd.DataFrame()\n    dt[\"date_id\"] = df[\"date_id\"]\n    for f in Features[1:]:\n        if f in target_lag_1:\n            lag = 1\n        elif f in target_lag_2:\n            lag = 2\n        elif f in target_lag_3:\n            lag = 3\n        elif f in target_lag_4:\n            lag = 4    \n        dt[f] = df[f].shift(lag).fillna(0)\n    return df, dt\n\n_, train_lagged = create_lagged_labels(trainl)\n\n# Create training data\nimport gc\ntraining_df = []\ntarget_cols = [f\"target_{i}\" for i in range(424)]\nfor j, target_col in enumerate(target_cols):\n    temp_train_df = pd.DataFrame()\n    temp_train_df[Features] = train_lagged[Features]                     \n    temp_train_df['target_id'] = j\n    y = trainl[target_col].values\n    temp_train_df['target'] = y\n    mask = ~(np.isnan(y) | np.isinf(y) | (np.abs(y) > 1e10))\n    training_df.append(temp_train_df[mask].copy())\n    del temp_train_df, y\n    gc.collect()\n\ntraining_df = pd.concat(training_df).reset_index(drop=True)\nFeatures2 = Features + [\"target_id\"]\nX_train = training_df[Features2]\ny_train = training_df[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2025-10-04T05:52:51.772210Z","iopub.execute_input":"2025-10-04T05:52:51.772478Z","iopub.status.idle":"2025-10-04T05:54:17.249997Z","shell.execute_reply.started":"2025-10-04T05:52:51.772455Z","shell.execute_reply":"2025-10-04T05:54:17.249219Z"},"papermill":{"duration":0.010266,"end_time":"2025-09-08T12:09:42.114913","exception":false,"start_time":"2025-09-08T12:09:42.104647","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Data preparation completed!\nTraining data shape: (831129, 426)\nFeatures: ['date_id', 'target_0', 'target_1', 'target_2', 'target_3', 'target_4', 'target_5', 'target_6', 'target_7', 'target_8', 'target_9', 'target_10', 'target_11', 'target_12', 'target_13', 'target_14', 'target_15', 'target_16', 'target_17', 'target_18', 'target_19', 'target_20', 'target_21', 'target_22', 'target_23', 'target_24', 'target_25', 'target_26', 'target_27', 'target_28', 'target_29', 'target_30', 'target_31', 'target_32', 'target_33', 'target_34', 'target_35', 'target_36', 'target_37', 'target_38', 'target_39', 'target_40', 'target_41', 'target_42', 'target_43', 'target_44', 'target_45', 'target_46', 'target_47', 'target_48', 'target_49', 'target_50', 'target_51', 'target_52', 'target_53', 'target_54', 'target_55', 'target_56', 'target_57', 'target_58', 'target_59', 'target_60', 'target_61', 'target_62', 'target_63', 'target_64', 'target_65', 'target_66', 'target_67', 'target_68', 'target_69', 'target_70', 'target_71', 'target_72', 'target_73', 'target_74', 'target_75', 'target_76', 'target_77', 'target_78', 'target_79', 'target_80', 'target_81', 'target_82', 'target_83', 'target_84', 'target_85', 'target_86', 'target_87', 'target_88', 'target_89', 'target_90', 'target_91', 'target_92', 'target_93', 'target_94', 'target_95', 'target_96', 'target_97', 'target_98', 'target_99', 'target_100', 'target_101', 'target_102', 'target_103', 'target_104', 'target_105', 'target_106', 'target_107', 'target_108', 'target_109', 'target_110', 'target_111', 'target_112', 'target_113', 'target_114', 'target_115', 'target_116', 'target_117', 'target_118', 'target_119', 'target_120', 'target_121', 'target_122', 'target_123', 'target_124', 'target_125', 'target_126', 'target_127', 'target_128', 'target_129', 'target_130', 'target_131', 'target_132', 'target_133', 'target_134', 'target_135', 'target_136', 'target_137', 'target_138', 'target_139', 'target_140', 'target_141', 'target_142', 'target_143', 'target_144', 'target_145', 'target_146', 'target_147', 'target_148', 'target_149', 'target_150', 'target_151', 'target_152', 'target_153', 'target_154', 'target_155', 'target_156', 'target_157', 'target_158', 'target_159', 'target_160', 'target_161', 'target_162', 'target_163', 'target_164', 'target_165', 'target_166', 'target_167', 'target_168', 'target_169', 'target_170', 'target_171', 'target_172', 'target_173', 'target_174', 'target_175', 'target_176', 'target_177', 'target_178', 'target_179', 'target_180', 'target_181', 'target_182', 'target_183', 'target_184', 'target_185', 'target_186', 'target_187', 'target_188', 'target_189', 'target_190', 'target_191', 'target_192', 'target_193', 'target_194', 'target_195', 'target_196', 'target_197', 'target_198', 'target_199', 'target_200', 'target_201', 'target_202', 'target_203', 'target_204', 'target_205', 'target_206', 'target_207', 'target_208', 'target_209', 'target_210', 'target_211', 'target_212', 'target_213', 'target_214', 'target_215', 'target_216', 'target_217', 'target_218', 'target_219', 'target_220', 'target_221', 'target_222', 'target_223', 'target_224', 'target_225', 'target_226', 'target_227', 'target_228', 'target_229', 'target_230', 'target_231', 'target_232', 'target_233', 'target_234', 'target_235', 'target_236', 'target_237', 'target_238', 'target_239', 'target_240', 'target_241', 'target_242', 'target_243', 'target_244', 'target_245', 'target_246', 'target_247', 'target_248', 'target_249', 'target_250', 'target_251', 'target_252', 'target_253', 'target_254', 'target_255', 'target_256', 'target_257', 'target_258', 'target_259', 'target_260', 'target_261', 'target_262', 'target_263', 'target_264', 'target_265', 'target_266', 'target_267', 'target_268', 'target_269', 'target_270', 'target_271', 'target_272', 'target_273', 'target_274', 'target_275', 'target_276', 'target_277', 'target_278', 'target_279', 'target_280', 'target_281', 'target_282', 'target_283', 'target_284', 'target_285', 'target_286', 'target_287', 'target_288', 'target_289', 'target_290', 'target_291', 'target_292', 'target_293', 'target_294', 'target_295', 'target_296', 'target_297', 'target_298', 'target_299', 'target_300', 'target_301', 'target_302', 'target_303', 'target_304', 'target_305', 'target_306', 'target_307', 'target_308', 'target_309', 'target_310', 'target_311', 'target_312', 'target_313', 'target_314', 'target_315', 'target_316', 'target_317', 'target_318', 'target_319', 'target_320', 'target_321', 'target_322', 'target_323', 'target_324', 'target_325', 'target_326', 'target_327', 'target_328', 'target_329', 'target_330', 'target_331', 'target_332', 'target_333', 'target_334', 'target_335', 'target_336', 'target_337', 'target_338', 'target_339', 'target_340', 'target_341', 'target_342', 'target_343', 'target_344', 'target_345', 'target_346', 'target_347', 'target_348', 'target_349', 'target_350', 'target_351', 'target_352', 'target_353', 'target_354', 'target_355', 'target_356', 'target_357', 'target_358', 'target_359', 'target_360', 'target_361', 'target_362', 'target_363', 'target_364', 'target_365', 'target_366', 'target_367', 'target_368', 'target_369', 'target_370', 'target_371', 'target_372', 'target_373', 'target_374', 'target_375', 'target_376', 'target_377', 'target_378', 'target_379', 'target_380', 'target_381', 'target_382', 'target_383', 'target_384', 'target_385', 'target_386', 'target_387', 'target_388', 'target_389', 'target_390', 'target_391', 'target_392', 'target_393', 'target_394', 'target_395', 'target_396', 'target_397', 'target_398', 'target_399', 'target_400', 'target_401', 'target_402', 'target_403', 'target_404', 'target_405', 'target_406', 'target_407', 'target_408', 'target_409', 'target_410', 'target_411', 'target_412', 'target_413', 'target_414', 'target_415', 'target_416', 'target_417', 'target_418', 'target_419', 'target_420', 'target_421', 'target_422', 'target_423', 'target_id']\n","output_type":"stream"}],"execution_count":1},{"id":"7f5373ca-44c7-4af2-a8f8-e582269338ab","cell_type":"code","source":"# Deeper Gradient Boosting models for regression\nxgb_model = XGBRegressor(\n    n_estimators=2000,\n    max_depth=6,\n    learning_rate=0.01,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=1,\n    reg_lambda=1,\n    random_state=42,\n    tree_method=\"hist\",\n    device=\"cuda\"\n)\n\nlgbm_model = LGBMRegressor(\n    n_estimators=2000,\n    max_depth=6,\n    learning_rate=0.01,\n    num_leaves=256,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=1,\n    reg_lambda=1,\n    random_state=42,\n    device=\"gpu\",\n    verbose=-1\n)\n\ncatboost_model = CatBoostRegressor(\n    iterations=2000,\n    depth=6,\n    learning_rate=0.01,\n    l2_leaf_reg=3,\n    random_seed=42,\n    loss_function='RMSE',\n    task_type=\"GPU\",\n    verbose=False\n)\n\n# Append models to a list for later training / ensembling\nmodels = [xgb_model, lgbm_model, catboost_model]\nModels = []\n\n# Train all models on the entire dataset (no target-specific training)\nprint(\"Training models on entire dataset...\")\nfor model in models:\n    model.fit(X_train, y_train)\n    Models.append(model)\n\nprint(f\"Models list created with {len(Models)} models.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T05:54:17.251175Z","iopub.execute_input":"2025-10-04T05:54:17.251550Z","iopub.status.idle":"2025-10-04T05:54:22.892003Z","shell.execute_reply.started":"2025-10-04T05:54:17.251530Z","shell.execute_reply":"2025-10-04T05:54:22.891446Z"}},"outputs":[],"execution_count":2},{"id":"c7b8642b-66c9-47f8-a3f5-405d79bbdfdd","cell_type":"code","source":"def ensemble_predict(models, X):\n    \"\"\"\n    Predict using a list of trained models and return the averaged prediction.\n    \n    Parameters:\n        models : list of trained models\n        X      : numpy array or DataFrame of features\n        \n    Returns:\n        ensemble_pred : averaged prediction across models\n    \"\"\"\n    preds = [model.predict(X) for model in models]\n    ensemble_pred = np.mean(preds, axis=0)\n    return ensemble_pred\n\n# Test the predictions\nX_data = X_train.copy()\nX_data[\"preds\"] = ensemble_predict(Models, X_train)\n\n# Convert to wide format (90 rows × 424 columns)\ndf_preds = X_data.copy()\ndf_preds['row'] = df_preds.groupby('target_id').cumcount()\n\n# Pivot the table to wide format\ndf_wide = df_preds.pivot(index='row', columns='target_id', values='preds')\ndf_wide = df_wide.sort_index(axis=1)\ndf_wide.index = [i for i in df_wide.index]\n\n# Rename columns\ndf_wide.columns = [f'target_{i}' for i in df_wide.columns]\nprint(f\"Wide format shape: {df_wide.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T05:56:53.843005Z","iopub.execute_input":"2025-10-04T05:56:53.843599Z","iopub.status.idle":"2025-10-04T05:56:53.854184Z","shell.execute_reply.started":"2025-10-04T05:56:53.843575Z","shell.execute_reply":"2025-10-04T05:56:53.853454Z"}},"outputs":[],"execution_count":6},{"id":"d4c4a2dd-0b41-4e93-839b-bb2c91059e59","cell_type":"code","source":"def rank_correlation_sharpe_ratio(merged_df: pd.DataFrame) -> float:\n    prediction_cols = [col for col in merged_df.columns if col.startswith('prediction_')]\n    target_cols = [col for col in merged_df.columns if col.startswith('target_')]\n    \n    def _compute_rank_correlation(row):\n        non_null_targets = [col for col in target_cols if not pd.isnull(row[col])]\n        matching_predictions = [col for col in prediction_cols if col.replace('prediction', 'target') in non_null_targets]\n        \n        if not non_null_targets:\n            raise ValueError('No non-null target values found')\n        if row[non_null_targets].std(ddof=0) == 0 or row[matching_predictions].std(ddof=0) == 0:\n            raise ZeroDivisionError('Denominator is zero, unable to compute rank correlation.')\n        \n        return np.corrcoef(\n            row[matching_predictions].rank(method='average'), \n            row[non_null_targets].rank(method='average')\n        )[0, 1]\n    \n    daily_rank_corrs = merged_df.apply(_compute_rank_correlation, axis=1)\n    std_dev = daily_rank_corrs.std(ddof=0)\n    \n    if std_dev == 0:\n        raise ZeroDivisionError('Denominator is zero, unable to compute Sharpe ratio.')\n    \n    sharpe_ratio = daily_rank_corrs.mean() / std_dev\n    return float(sharpe_ratio)\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame) -> float:\n    assert all(solution.columns == submission.columns)\n    submission = submission.rename(columns={col: col.replace('target_', 'prediction_') for col in submission.columns})\n    solution = solution.replace(0, None)\n    return rank_correlation_sharpe_ratio(pd.concat([solution, submission], axis='columns'))\n\n# Test scoring\nscore_value = score(trainl[Features[1:]], df_wide[Features[1:]])\nprint(f\"SCORE: {score_value:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T05:59:29.550667Z","iopub.execute_input":"2025-10-04T05:59:29.551330Z","iopub.status.idle":"2025-10-04T05:59:29.557057Z","shell.execute_reply.started":"2025-10-04T05:59:29.551308Z","shell.execute_reply":"2025-10-04T05:59:29.556332Z"}},"outputs":[],"execution_count":10},{"id":"e9964e11-9bee-4816-9605-2995e455e6ee","cell_type":"code","source":"import polars as pl\n\ndef predict(\n    test: pl.DataFrame,\n    lag1: pl.DataFrame, \n    lag2: pl.DataFrame,\n    lag3: pl.DataFrame,\n    lag4: pl.DataFrame,\n) -> pl.DataFrame:\n    \"\"\"\n    Predicts target values using lag features.\n    This is your working version from the notebook.\n    \"\"\"\n    # Convert to pandas\n    test_pd = test.to_pandas()\n    lag1_pd = lag1.to_pandas()\n    lag2_pd = lag2.to_pandas()\n    lag3_pd = lag3.to_pandas()\n    lag4_pd = lag4.to_pandas()\n    \n    # Combine lag features\n    X_pred = pd.concat([\n        test_pd[[\"date_id\"]],\n        lag1_pd[target_lag_1],\n        lag2_pd[target_lag_2],\n        lag3_pd[target_lag_3],\n        lag4_pd[target_lag_4],\n    ], axis=1)\n    \n    # If no rows, return all zeros\n    if X_pred.height == 0:\n        return pl.DataFrame(0, schema=[(f\"target_{i}\", pl.Float64) for i in range(424)])\n    \n    # Fill nulls with 0\n    X_pred = X_pred.fillna(0)\n    \n    # Prepare features for prediction\n    n_targets = 424\n    n_rows = X_pred.shape[0]\n    \n    # Create features for all targets\n    features_array = np.tile(X_pred[Features[1:]].values, (n_targets, 1))\n    target_ids = np.repeat(np.arange(n_targets), n_rows)\n    \n    # Create prediction DataFrame\n    X_pred2 = pd.DataFrame({\n        \"date_id\": np.tile(X_pred[\"date_id\"].values, n_targets),\n        **{feat: features_array[:, i] for i, feat in enumerate(Features[1:])},\n        \"target_id\": target_ids,\n        \"row\": np.tile(np.arange(n_rows), n_targets)\n    })\n    \n    # Make predictions\n    preds = ensemble_predict(Models, X_pred2[Features2])\n    X_pred2 = X_pred2.assign(preds=preds)\n    \n    # Pivot to wide format\n    df_wide = (\n        X_pred2.groupby([\"target_id\", \"row\"])\n        .agg({\"preds\": \"first\"})\n        .reset_index()\n        .pivot(index=\"row\", columns=\"target_id\", values=\"preds\")\n        .sort_index()\n    )\n    \n    # Ensure correct column order\n    df_wide = df_wide.reindex(columns=range(424), fill_value=0)\n    df_wide.columns = [f\"target_{i}\" for i in range(424)]\n    \n    # Return last row as predictions\n    result_df = df_wide.tail(1)\n    return pl.from_pandas(result_df.reset_index(drop=True))\n\n# Test the prediction function\ndef test_prediction():\n    \"\"\"Test the prediction function\"\"\"\n    sample_test = pl.from_pandas(trainl[Features].iloc[:5])\n    sample_lag1 = pl.from_pandas(trainl[target_lag_1].iloc[:5])\n    sample_lag2 = pl.from_pandas(trainl[target_lag_2].iloc[:5])\n    sample_lag3 = pl.from_pandas(trainl[target_lag_3].iloc[:5])\n    sample_lag4 = pl.from_pandas(trainl[target_lag_4].iloc[:5])\n    \n    result = predict(sample_test, sample_lag1, sample_lag2, sample_lag3, sample_lag4)\n    print(f\"Prediction result shape: {result.shape}\")\n    return result\n\ntest_result = test_prediction()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T05:59:59.791944Z","iopub.execute_input":"2025-10-04T05:59:59.792217Z","iopub.status.idle":"2025-10-04T05:59:59.808309Z","shell.execute_reply.started":"2025-10-04T05:59:59.792193Z","shell.execute_reply":"2025-10-04T05:59:59.807654Z"}},"outputs":[{"name":"stdout","text":"Save/load functions ready!\n","output_type":"stream"}],"execution_count":12},{"id":"67b4c687-d8eb-494b-a56e-4c57c3218535","cell_type":"code","source":"import joblib\n\ndef save_models():\n    \"\"\"Save the trained models\"\"\"\n    joblib.dump(Models, '/kaggle/working/models_list.joblib')\n    print(f\"Saved {len(Models)} models\")\n\ndef load_models():\n    \"\"\"Load the trained models\"\"\"\n    Models = joblib.load('/kaggle/working/models_list.joblib')\n    print(f\"Loaded {len(Models)} models\")\n    return Models\n\n# Save models\nsave_models()\n\n# For competition submission\nimport os\nimport kaggle_evaluation.mitsui_inference_server\n\ninference_server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/mitsui-commodity-prediction-challenge/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T05:59:37.051940Z","iopub.execute_input":"2025-10-04T05:59:37.052493Z","iopub.status.idle":"2025-10-04T05:59:53.757280Z","shell.execute_reply.started":"2025-10-04T05:59:37.052472Z","shell.execute_reply":"2025-10-04T05:59:53.756591Z"}},"outputs":[{"name":"stdout","text":"=== TESTING POLARS PREDICTION ===\nInput types:\ntest: <class 'polars.dataframe.frame.DataFrame'>\nlag1: <class 'polars.dataframe.frame.DataFrame'>\nlag2: <class 'polars.dataframe.frame.DataFrame'>\nlag3: <class 'polars.dataframe.frame.DataFrame'>\nlag4: <class 'polars.dataframe.frame.DataFrame'>\nInput shapes - test: (5, 425), lag1: (5, 106), lag2: (5, 106), lag3: (5, 106), lag4: (5, 106)\nModels list loaded from /kaggle/input/mitsuienslearning/models_list.joblib with 1272 models\nLoaded 1272 models for prediction\nCreating features for 5 rows and 424 targets...\nPrediction DataFrame shape: (2120, 427)\nPredictions completed, min: -0.0002, max: 0.0000, mean: -0.0002\nWide format shape: (5, 424)\nFinal result shape: (1, 424)\nResult type: <class 'polars.dataframe.frame.DataFrame'>\nResult shape: (1, 424)\nResult columns: ['target_0', 'target_1', 'target_2', 'target_3', 'target_4', 'target_5', 'target_6', 'target_7', 'target_8', 'target_9']...\nPrediction block ready! Use 'competition_predict' for submission.\n","output_type":"stream"}],"execution_count":11},{"id":"48959ca6","cell_type":"code","source":"# submission through the API\nimport os\nimport kaggle_evaluation.mitsui_inference_server\n\ninference_server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print('there')\n    # inference_server.serve()\nelse:\n    print('here')\n    inference_server.run_local_gateway(('/kaggle/input/mitsui-commodity-prediction-challenge/',))","metadata":{"papermill":{"duration":49.005971,"end_time":"2025-09-08T12:22:13.642777","exception":false,"start_time":"2025-09-08T12:21:24.636806","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e9a1959c","cell_type":"code","source":"display(pl.read_parquet('/kaggle/working/submission.parquet'))","metadata":{"papermill":{"duration":0.141377,"end_time":"2025-09-08T12:22:13.791255","exception":false,"start_time":"2025-09-08T12:22:13.649878","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T06:26:32.239436Z","iopub.execute_input":"2025-10-04T06:26:32.239934Z","iopub.status.idle":"2025-10-04T06:26:32.292814Z","shell.execute_reply.started":"2025-10-04T06:26:32.239909Z","shell.execute_reply":"2025-10-04T06:26:32.292178Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"shape: (134, 425)\n┌─────────┬───────────┬───────────┬───────────┬───┬────────────┬───────────┬───────────┬───────────┐\n│ date_id ┆ target_0  ┆ target_1  ┆ target_2  ┆ … ┆ target_420 ┆ target_42 ┆ target_42 ┆ target_42 │\n│ ---     ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---        ┆ 1         ┆ 2         ┆ 3         │\n│ i64     ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64        ┆ ---       ┆ ---       ┆ ---       │\n│         ┆           ┆           ┆           ┆   ┆            ┆ f64       ┆ f64       ┆ f64       │\n╞═════════╪═══════════╪═══════════╪═══════════╪═══╪════════════╪═══════════╪═══════════╪═══════════╡\n│ 1827    ┆ -0.000068 ┆ -0.000068 ┆ -0.000068 ┆ … ┆ -0.000068  ┆ -0.000068 ┆ -0.000068 ┆ -0.000068 │\n│ 1828    ┆ -0.000068 ┆ -0.000068 ┆ -0.000068 ┆ … ┆ -0.000068  ┆ -0.000068 ┆ -0.000068 ┆ -0.000068 │\n│ 1829    ┆ -0.000136 ┆ -0.000136 ┆ -0.000136 ┆ … ┆ -0.000136  ┆ -0.000136 ┆ -0.000136 ┆ -0.000136 │\n│ 1830    ┆ -0.000235 ┆ -0.000235 ┆ -0.000235 ┆ … ┆ -0.000235  ┆ -0.000235 ┆ -0.000235 ┆ -0.000235 │\n│ 1831    ┆ -0.000026 ┆ -0.000026 ┆ -0.000026 ┆ … ┆ -0.000026  ┆ -0.000026 ┆ -0.000026 ┆ -0.000026 │\n│ …       ┆ …         ┆ …         ┆ …         ┆ … ┆ …          ┆ …         ┆ …         ┆ …         │\n│ 1956    ┆ 0.000014  ┆ 0.000014  ┆ 0.000014  ┆ … ┆ 0.000014   ┆ 0.000014  ┆ 0.000014  ┆ 0.000014  │\n│ 1957    ┆ -0.000012 ┆ -0.000012 ┆ -0.000012 ┆ … ┆ -0.000012  ┆ -0.000012 ┆ -0.000012 ┆ -0.000012 │\n│ 1958    ┆ 0.000016  ┆ 0.000016  ┆ 0.000016  ┆ … ┆ 0.000016   ┆ 0.000016  ┆ 0.000016  ┆ 0.000016  │\n│ 1959    ┆ 0.000016  ┆ 0.000016  ┆ 0.000016  ┆ … ┆ 0.000016   ┆ 0.000016  ┆ 0.000016  ┆ 0.000016  │\n│ 1960    ┆ 0.000035  ┆ 0.000035  ┆ 0.000035  ┆ … ┆ 0.000035   ┆ 0.000035  ┆ 0.000035  ┆ 0.000035  │\n└─────────┴───────────┴───────────┴───────────┴───┴────────────┴───────────┴───────────┴───────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (134, 425)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>target_0</th><th>target_1</th><th>target_2</th><th>target_3</th><th>target_4</th><th>target_5</th><th>target_6</th><th>target_7</th><th>target_8</th><th>target_9</th><th>target_10</th><th>target_11</th><th>target_12</th><th>target_13</th><th>target_14</th><th>target_15</th><th>target_16</th><th>target_17</th><th>target_18</th><th>target_19</th><th>target_20</th><th>target_21</th><th>target_22</th><th>target_23</th><th>target_24</th><th>target_25</th><th>target_26</th><th>target_27</th><th>target_28</th><th>target_29</th><th>target_30</th><th>target_31</th><th>target_32</th><th>target_33</th><th>target_34</th><th>target_35</th><th>&hellip;</th><th>target_387</th><th>target_388</th><th>target_389</th><th>target_390</th><th>target_391</th><th>target_392</th><th>target_393</th><th>target_394</th><th>target_395</th><th>target_396</th><th>target_397</th><th>target_398</th><th>target_399</th><th>target_400</th><th>target_401</th><th>target_402</th><th>target_403</th><th>target_404</th><th>target_405</th><th>target_406</th><th>target_407</th><th>target_408</th><th>target_409</th><th>target_410</th><th>target_411</th><th>target_412</th><th>target_413</th><th>target_414</th><th>target_415</th><th>target_416</th><th>target_417</th><th>target_418</th><th>target_419</th><th>target_420</th><th>target_421</th><th>target_422</th><th>target_423</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1827</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>&hellip;</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td></tr><tr><td>1828</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>&hellip;</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td></tr><tr><td>1829</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>&hellip;</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td></tr><tr><td>1830</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>&hellip;</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td></tr><tr><td>1831</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>&hellip;</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1956</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>&hellip;</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td></tr><tr><td>1957</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>&hellip;</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td></tr><tr><td>1958</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>&hellip;</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td></tr><tr><td>1959</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>&hellip;</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td></tr><tr><td>1960</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>&hellip;</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":16},{"id":"92645eab","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.00639,"end_time":"2025-09-08T12:22:13.818388","exception":false,"start_time":"2025-09-08T12:22:13.811998","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}