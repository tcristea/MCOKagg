{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94771,"databundleVersionId":13613251,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13256549,"sourceType":"datasetVersion","datasetId":8400276}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":765.21753,"end_time":"2025-09-08T12:22:14.944658","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-08T12:09:29.727128","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bade4ff9","cell_type":"markdown","source":"---\n\n**In this notebook, we will use Ensembling Gradient Boosting trees for a multi-target regression problem, leveraging lagged targets to predict 424 outputs. To speed up inference, we adopt the long-format multi-output prediction method, which is much faster than the standard multi-output approach.**\n\n**You will also find several useful techniques in this notebook, including:**\n\n* How to create lagged targets and use them in the prediction step\n* How to run Ensembling Gradient Boosting trees with lags targets\n* How to build an optimized prediction function for API inference\n\n\n\n---","metadata":{"papermill":{"duration":0.00386,"end_time":"2025-09-08T12:09:35.266874","exception":false,"start_time":"2025-09-08T12:09:35.263014","status":"completed"},"tags":[]}},{"id":"c65a313c","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle\nimport joblib\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Data loading and preprocessing\np = '/kaggle/input/mitsui-commodity-prediction-challenge/'\ntrain = pd.read_csv(p+'train.csv')\ntrainl = pd.read_csv(p+'train_labels.csv')\ntraint = pd.read_csv(p+'target_pairs.csv')\n\ndef _handle_missing_values(data):\n    \"\"\"Improved missing value handling\"\"\"\n    data.interpolate(method='polynomial', order=3, inplace=True)\n    data.clip(lower=-10, upper=10, inplace=True)\n    return data\n\ntrain = _handle_missing_values(train)\ntrainl = _handle_missing_values(trainl)\n\ntarget_lag_1 = traint.loc[traint[\"lag\"]==1,\"target\"].values\ntarget_lag_2 = traint.loc[traint[\"lag\"]==2,\"target\"].values\ntarget_lag_3 = traint.loc[traint[\"lag\"]==3,\"target\"].values\ntarget_lag_4 = traint.loc[traint[\"lag\"]==4,\"target\"].values\n\nFeatures = [i for i in trainl.columns]\n\ndef create_lagged_labels(df):\n    dt = pd.DataFrame()\n    dt[\"date_id\"] = df[\"date_id\"]\n    for f in Features[1:]:\n        if f in target_lag_1:\n            lag = 1\n        elif f in target_lag_2:\n            lag = 2\n        elif f in target_lag_3:\n            lag = 3\n        elif f in target_lag_4:\n            lag = 4    \n        dt[f] = df[f].shift(lag).fillna(0)\n    return df, dt\n\n_, train_lagged = create_lagged_labels(trainl)\n\n# Create training data in long format\nimport gc\ntraining_df = []\ntarget_cols = [f\"target_{i}\" for i in range(424)]\nfor j, target_col in enumerate(target_cols):\n    temp_train_df = pd.DataFrame()\n    temp_train_df[Features] = train_lagged[Features]                     \n    temp_train_df['target_id'] = j\n    y = trainl[target_col].values\n    temp_train_df['target'] = y\n    mask = ~(np.isnan(y) | np.isinf(y) | (np.abs(y) > 1e10))\n    training_df.append(temp_train_df[mask].copy())\n    del temp_train_df, y\n    gc.collect()\n\ntraining_df = pd.concat(training_df).reset_index(drop=True)\nFeatures2 = Features + [\"target_id\"]\nX_train = training_df[Features2]\ny_train = training_df[\"target\"]\n\nprint(\"Data preparation completed!\")\nprint(f\"Training data shape: {X_train.shape}\")\nprint(f\"Features: {Features2}\")","metadata":{"execution":{"iopub.status.busy":"2025-10-04T05:52:51.772210Z","iopub.execute_input":"2025-10-04T05:52:51.772478Z","iopub.status.idle":"2025-10-04T05:54:17.249997Z","shell.execute_reply.started":"2025-10-04T05:52:51.772455Z","shell.execute_reply":"2025-10-04T05:54:17.249219Z"},"papermill":{"duration":0.010266,"end_time":"2025-09-08T12:09:42.114913","exception":false,"start_time":"2025-09-08T12:09:42.104647","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Data preparation completed!\nTraining data shape: (831129, 426)\nFeatures: ['date_id', 'target_0', 'target_1', 'target_2', 'target_3', 'target_4', 'target_5', 'target_6', 'target_7', 'target_8', 'target_9', 'target_10', 'target_11', 'target_12', 'target_13', 'target_14', 'target_15', 'target_16', 'target_17', 'target_18', 'target_19', 'target_20', 'target_21', 'target_22', 'target_23', 'target_24', 'target_25', 'target_26', 'target_27', 'target_28', 'target_29', 'target_30', 'target_31', 'target_32', 'target_33', 'target_34', 'target_35', 'target_36', 'target_37', 'target_38', 'target_39', 'target_40', 'target_41', 'target_42', 'target_43', 'target_44', 'target_45', 'target_46', 'target_47', 'target_48', 'target_49', 'target_50', 'target_51', 'target_52', 'target_53', 'target_54', 'target_55', 'target_56', 'target_57', 'target_58', 'target_59', 'target_60', 'target_61', 'target_62', 'target_63', 'target_64', 'target_65', 'target_66', 'target_67', 'target_68', 'target_69', 'target_70', 'target_71', 'target_72', 'target_73', 'target_74', 'target_75', 'target_76', 'target_77', 'target_78', 'target_79', 'target_80', 'target_81', 'target_82', 'target_83', 'target_84', 'target_85', 'target_86', 'target_87', 'target_88', 'target_89', 'target_90', 'target_91', 'target_92', 'target_93', 'target_94', 'target_95', 'target_96', 'target_97', 'target_98', 'target_99', 'target_100', 'target_101', 'target_102', 'target_103', 'target_104', 'target_105', 'target_106', 'target_107', 'target_108', 'target_109', 'target_110', 'target_111', 'target_112', 'target_113', 'target_114', 'target_115', 'target_116', 'target_117', 'target_118', 'target_119', 'target_120', 'target_121', 'target_122', 'target_123', 'target_124', 'target_125', 'target_126', 'target_127', 'target_128', 'target_129', 'target_130', 'target_131', 'target_132', 'target_133', 'target_134', 'target_135', 'target_136', 'target_137', 'target_138', 'target_139', 'target_140', 'target_141', 'target_142', 'target_143', 'target_144', 'target_145', 'target_146', 'target_147', 'target_148', 'target_149', 'target_150', 'target_151', 'target_152', 'target_153', 'target_154', 'target_155', 'target_156', 'target_157', 'target_158', 'target_159', 'target_160', 'target_161', 'target_162', 'target_163', 'target_164', 'target_165', 'target_166', 'target_167', 'target_168', 'target_169', 'target_170', 'target_171', 'target_172', 'target_173', 'target_174', 'target_175', 'target_176', 'target_177', 'target_178', 'target_179', 'target_180', 'target_181', 'target_182', 'target_183', 'target_184', 'target_185', 'target_186', 'target_187', 'target_188', 'target_189', 'target_190', 'target_191', 'target_192', 'target_193', 'target_194', 'target_195', 'target_196', 'target_197', 'target_198', 'target_199', 'target_200', 'target_201', 'target_202', 'target_203', 'target_204', 'target_205', 'target_206', 'target_207', 'target_208', 'target_209', 'target_210', 'target_211', 'target_212', 'target_213', 'target_214', 'target_215', 'target_216', 'target_217', 'target_218', 'target_219', 'target_220', 'target_221', 'target_222', 'target_223', 'target_224', 'target_225', 'target_226', 'target_227', 'target_228', 'target_229', 'target_230', 'target_231', 'target_232', 'target_233', 'target_234', 'target_235', 'target_236', 'target_237', 'target_238', 'target_239', 'target_240', 'target_241', 'target_242', 'target_243', 'target_244', 'target_245', 'target_246', 'target_247', 'target_248', 'target_249', 'target_250', 'target_251', 'target_252', 'target_253', 'target_254', 'target_255', 'target_256', 'target_257', 'target_258', 'target_259', 'target_260', 'target_261', 'target_262', 'target_263', 'target_264', 'target_265', 'target_266', 'target_267', 'target_268', 'target_269', 'target_270', 'target_271', 'target_272', 'target_273', 'target_274', 'target_275', 'target_276', 'target_277', 'target_278', 'target_279', 'target_280', 'target_281', 'target_282', 'target_283', 'target_284', 'target_285', 'target_286', 'target_287', 'target_288', 'target_289', 'target_290', 'target_291', 'target_292', 'target_293', 'target_294', 'target_295', 'target_296', 'target_297', 'target_298', 'target_299', 'target_300', 'target_301', 'target_302', 'target_303', 'target_304', 'target_305', 'target_306', 'target_307', 'target_308', 'target_309', 'target_310', 'target_311', 'target_312', 'target_313', 'target_314', 'target_315', 'target_316', 'target_317', 'target_318', 'target_319', 'target_320', 'target_321', 'target_322', 'target_323', 'target_324', 'target_325', 'target_326', 'target_327', 'target_328', 'target_329', 'target_330', 'target_331', 'target_332', 'target_333', 'target_334', 'target_335', 'target_336', 'target_337', 'target_338', 'target_339', 'target_340', 'target_341', 'target_342', 'target_343', 'target_344', 'target_345', 'target_346', 'target_347', 'target_348', 'target_349', 'target_350', 'target_351', 'target_352', 'target_353', 'target_354', 'target_355', 'target_356', 'target_357', 'target_358', 'target_359', 'target_360', 'target_361', 'target_362', 'target_363', 'target_364', 'target_365', 'target_366', 'target_367', 'target_368', 'target_369', 'target_370', 'target_371', 'target_372', 'target_373', 'target_374', 'target_375', 'target_376', 'target_377', 'target_378', 'target_379', 'target_380', 'target_381', 'target_382', 'target_383', 'target_384', 'target_385', 'target_386', 'target_387', 'target_388', 'target_389', 'target_390', 'target_391', 'target_392', 'target_393', 'target_394', 'target_395', 'target_396', 'target_397', 'target_398', 'target_399', 'target_400', 'target_401', 'target_402', 'target_403', 'target_404', 'target_405', 'target_406', 'target_407', 'target_408', 'target_409', 'target_410', 'target_411', 'target_412', 'target_413', 'target_414', 'target_415', 'target_416', 'target_417', 'target_418', 'target_419', 'target_420', 'target_421', 'target_422', 'target_423', 'target_id']\n","output_type":"stream"}],"execution_count":1},{"id":"7f5373ca-44c7-4af2-a8f8-e582269338ab","cell_type":"code","source":"from xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nclass MultiTargetEnsemble:\n    def __init__(self, use_gpu=True, target_cols=None, quick_test=False):\n        self.use_gpu = use_gpu\n        self.target_cols = target_cols or [f\"target_{i}\" for i in range(424)]\n        self.models = {}\n        self.is_fitted = False\n        self.feature_names = []\n        self.training_params = {}\n        self.quick_test = quick_test\n        \n    def _initialize_model(self, model_type):\n        \"\"\"Initialize individual model with optimized parameters\"\"\"\n        if self.quick_test:\n            base_params = {\n                'n_estimators': 10,\n                'learning_rate': 0.1,\n                'random_state': 42,\n                'early_stopping_rounds': 5\n            }\n        else:\n            base_params = {\n                'n_estimators': 2000,\n                'learning_rate': 0.01,\n                'random_state': 42,\n                'early_stopping_rounds': 50\n            }\n        \n        if model_type == 'xgb':\n            return XGBRegressor(\n                **base_params,\n                max_depth=6 if self.quick_test else 8,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                reg_alpha=1,\n                reg_lambda=1,\n                tree_method=\"hist\" if self.use_gpu else \"auto\",\n                device=\"cuda\" if self.use_gpu else \"cpu\",\n                eval_metric='rmse'\n            )\n        elif model_type == 'lgbm':\n            return LGBMRegressor(\n                **base_params,\n                max_depth=6 if self.quick_test else 8,\n                num_leaves=32 if self.quick_test else 128,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                reg_alpha=1,\n                reg_lambda=1,\n                device=\"gpu\" if self.use_gpu else \"cpu\",\n                n_jobs=-1,\n                verbose=-1\n            )\n        elif model_type == 'catboost':\n            return CatBoostRegressor(\n                iterations=base_params['n_estimators'],\n                learning_rate=base_params['learning_rate'],\n                random_seed=base_params['random_state'],\n                early_stopping_rounds=base_params['early_stopping_rounds'],\n                depth=6 if self.quick_test else 8,\n                l2_leaf_reg=3,\n                task_type=\"GPU\" if self.use_gpu else \"CPU\",\n                verbose=False,\n                loss_function='RMSE'\n            )\n    \n    def prepare_validation_data(self, X, y, validation_size=0.2):\n        \"\"\"Prepare validation data maintaining temporal structure\"\"\"\n        n_samples = len(X)\n        split_idx = int(n_samples * (1 - validation_size))\n        return X.iloc[:split_idx], X.iloc[split_idx:], y.iloc[:split_idx], y.iloc[split_idx:]\n    \n    def fit_individual_target(self, target_id, X_train, y_train, X_val=None, y_val=None):\n        \"\"\"Train models for a specific target\"\"\"\n        target_models = {}\n        \n        for model_type in ['xgb', 'lgbm', 'catboost']:\n            if self.quick_test:\n                print(f\"Quick training {model_type} for target {target_id}...\")\n            else:\n                print(f\"Training {model_type} for target {target_id}...\")\n            \n            model = self._initialize_model(model_type)\n            \n            # Prepare evaluation set\n            eval_set = None\n            if X_val is not None and y_val is not None:\n                if model_type == 'catboost':\n                    eval_set = [(X_val, y_val)]\n                else:\n                    eval_set = [(X_train, y_train), (X_val, y_val)]\n            \n            verbose_level = 10 if self.quick_test else 100\n            \n            try:\n                if model_type == 'xgb':\n                    model.fit(X_train, y_train, eval_set=eval_set, verbose=verbose_level)\n                elif model_type == 'lgbm':\n                    model.fit(X_train, y_train, eval_set=eval_set, eval_metric='rmse')\n                elif model_type == 'catboost':\n                    model.fit(X_train, y_train, eval_set=eval_set, verbose=verbose_level)\n                \n                target_models[model_type] = model\n            except Exception as e:\n                print(f\"Error training {model_type} for target {target_id}: {e}\")\n                continue\n        \n        return target_models\n    \n    def fit(self, X, y, target_ids, validation_size=0.2, max_targets=None):\n        \"\"\"Train models for all targets\"\"\"\n        self.feature_names = X.columns.tolist() if hasattr(X, 'columns') else []\n        \n        self.training_params = {\n            'feature_names': self.feature_names,\n            'validation_size': validation_size,\n            'n_targets': target_ids.nunique(),\n            'use_gpu': self.use_gpu,\n            'quick_test': self.quick_test\n        }\n        \n        unique_targets = target_ids.unique()\n        \n        # Limit targets for quick testing\n        if max_targets and len(unique_targets) > max_targets:\n            print(f\"Quick test: Limiting to {max_targets} targets out of {len(unique_targets)}\")\n            unique_targets = unique_targets[:max_targets]\n        \n        for i, target_id in enumerate(unique_targets):\n            if self.quick_test:\n                print(f\"Quick training target {target_id} ({i+1}/{len(unique_targets)})\")\n            else:\n                print(f\"Training target {target_id} ({i+1}/{len(unique_targets)})\")\n            \n            # Filter data for this target\n            mask = target_ids == target_id\n            X_target = X[mask]\n            y_target = y[mask]\n            \n            if len(X_target) == 0:\n                continue\n            \n            # For quick test, use smaller subset\n            if self.quick_test and len(X_target) > 1000:\n                X_target = X_target.iloc[:1000]\n                y_target = y_target.iloc[:1000]\n            \n            # Split data\n            X_train, X_val, y_train, y_val = self.prepare_validation_data(X_target, y_target, validation_size)\n            \n            # Train models for this target\n            self.models[target_id] = self.fit_individual_target(target_id, X_train, y_train, X_val, y_val)\n        \n        self.is_fitted = True\n        print(f\"Training completed for {len(unique_targets)} targets\")\n        return self\n\n# Quick test function\ndef quick_test():\n    \"\"\"Run quick end-to-end test\"\"\"\n    print(\"=== QUICK E2E TEST ===\")\n    ensemble = MultiTargetEnsemble(use_gpu=True, quick_test=True)\n    \n    # Use only first 2 targets for quick testing\n    test_targets = training_df[\"target_id\"].unique()[:2]\n    test_mask = training_df[\"target_id\"].isin(test_targets)\n    test_data = training_df[test_mask].iloc[:500]\n    \n    ensemble.fit(\n        X=test_data[Features2],\n        y=test_data[\"target\"],\n        target_ids=test_data[\"target_id\"],\n        validation_size=0.2,\n        max_targets=2\n    )\n    \n    print(\"Quick test completed!\")\n    return ensemble\n\n# Full training function  \ndef full_training():\n    \"\"\"Run full training\"\"\"\n    print(\"=== FULL TRAINING ===\")\n    ensemble = MultiTargetEnsemble(use_gpu=True, quick_test=False)\n    \n    ensemble.fit(\n        X=training_df[Features2],\n        y=training_df[\"target\"],\n        target_ids=training_df[\"target_id\"],\n        validation_size=0.2\n    )\n    \n    print(\"Full training completed!\")\n    return ensemble\n\n# Run quick test first\n# ensemble = quick_test()\n# ensemble = full_training()  # Uncomment for full training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T05:54:17.251175Z","iopub.execute_input":"2025-10-04T05:54:17.251550Z","iopub.status.idle":"2025-10-04T05:54:22.892003Z","shell.execute_reply.started":"2025-10-04T05:54:17.251530Z","shell.execute_reply":"2025-10-04T05:54:22.891446Z"}},"outputs":[],"execution_count":2},{"id":"c7b8642b-66c9-47f8-a3f5-405d79bbdfdd","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\ndef evaluate_with_competition_metric_fixed(model, X_test, y_test, target_ids_test, solution_template):\n    \"\"\"Evaluate model using the competition metric - FIXED VERSION\"\"\"\n    \n    try:\n        # Make predictions using your working format\n        predictions_df = model.predict_multi_target(X_test, target_ids_test)\n        \n        if predictions_df.empty:\n            print(\"Warning: No predictions generated\")\n            return -1\n        \n        # Convert to wide format like your working code\n        df_preds = predictions_df.copy()\n        df_preds['row'] = df_preds.groupby('target_id').cumcount()\n        \n        # Pivot to wide format (90 rows × 424 columns)\n        df_wide = df_preds.pivot(index='row', columns='target_id', values='prediction')\n        \n        # Sort columns and rename\n        df_wide = df_wide.sort_index(axis=1)\n        df_wide.columns = [f'target_{int(col)}' for col in df_wide.columns]\n        \n        # Ensure we have all 424 targets\n        for i in range(424):\n            col_name = f'target_{i}'\n            if col_name not in df_wide.columns:\n                df_wide[col_name] = 0.0\n        \n        # Reorder columns to match solution format\n        target_cols = [f'target_{i}' for i in range(424)]\n        df_wide = df_wide[target_cols]\n        \n        print(f\"Prediction shape: {df_wide.shape}\")\n        \n        # Prepare solution data in the same format\n        # We need to extract the corresponding rows from solution_template\n        unique_rows = df_wide.index.unique()\n        \n        # Create solution subset with same row structure\n        solution_subset = solution_template.iloc[unique_rows][target_cols].reset_index(drop=True)\n        \n        print(f\"Solution shape: {solution_subset.shape}\")\n        print(f\"Submission shape: {df_wide.shape}\")\n        \n        # Calculate score using your working function\n        score_value = score(solution_subset, df_wide)\n        print(f\"Competition score: {score_value:.6f}\")\n        return score_value\n        \n    except Exception as e:\n        print(f\"Error calculating competition score: {e}\")\n        import traceback\n        traceback.print_exc()\n        return -1\n\n# Alternative simple scoring for quick test\ndef quick_score(model, X_test, y_test, target_ids_test, solution_template, n_samples=90):\n    \"\"\"Simple scoring for quick testing\"\"\"\n    try:\n        # Use only first n_samples for quick testing\n        test_subset = X_test.iloc[:n_samples]\n        target_ids_subset = target_ids_test.iloc[:n_samples]\n        \n        # Make predictions\n        predictions_df = model.predict_multi_target(test_subset, target_ids_subset)\n        \n        if predictions_df.empty:\n            return 0.0\n            \n        # Convert to wide format\n        df_preds = predictions_df.copy()\n        df_preds['row'] = df_preds.groupby('target_id').cumcount()\n        df_wide = df_preds.pivot(index='row', columns='target_id', values='prediction')\n        df_wide = df_wide.sort_index(axis=1)\n        df_wide.columns = [f'target_{int(col)}' for col in df_wide.columns]\n        \n        # Fill missing targets\n        for i in range(424):\n            col_name = f'target_{i}'\n            if col_name not in df_wide.columns:\n                df_wide[col_name] = 0.0\n        \n        target_cols = [f'target_{i}' for i in range(424)]\n        df_wide = df_wide[target_cols]\n        \n        # Get corresponding solution data\n        solution_subset = solution_template.iloc[:len(df_wide)][target_cols].reset_index(drop=True)\n        \n        # Calculate score\n        return score(solution_subset, df_wide)\n        \n    except Exception as e:\n        print(f\"Quick score error: {e}\")\n        return 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T05:56:53.843005Z","iopub.execute_input":"2025-10-04T05:56:53.843599Z","iopub.status.idle":"2025-10-04T05:56:53.854184Z","shell.execute_reply.started":"2025-10-04T05:56:53.843575Z","shell.execute_reply":"2025-10-04T05:56:53.853454Z"}},"outputs":[],"execution_count":6},{"id":"d4c4a2dd-0b41-4e93-839b-bb2c91059e59","cell_type":"code","source":"# Update the ensemble_predict functions to ensure they work\ndef ensemble_predict_from_list(models_list, X):\n    \"\"\"\n    Original ensemble_predict function that works with a list of models.\n    Handles both DataFrame and array inputs.\n    \"\"\"\n    if hasattr(X, 'values'):\n        X_values = X.values\n    else:\n        X_values = X\n        \n    preds = []\n    for i, model in enumerate(models_list):\n        try:\n            pred = model.predict(X_values)\n            preds.append(pred)\n        except Exception as e:\n            print(f\"Model {i} prediction error: {e}\")\n            # Add zeros if model fails\n            preds.append(np.zeros(len(X_values)))\n    \n    if not preds:\n        print(\"Warning: No successful predictions\")\n        return np.zeros(len(X_values))\n    \n    ensemble_pred = np.mean(preds, axis=0)\n    return ensemble_pred\n\ndef ensemble_predict(models_dict, X):\n    \"\"\"\n    Predict using a dictionary of trained models.\n    \"\"\"\n    # Convert models dictionary to a flat list\n    models_list = []\n    for target_models in models_dict.values():\n        models_list.extend(list(target_models.values()))\n    \n    print(f\"Making predictions with {len(models_list)} models\")\n    return ensemble_predict_from_list(models_list, X)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T05:59:29.550667Z","iopub.execute_input":"2025-10-04T05:59:29.551330Z","iopub.status.idle":"2025-10-04T05:59:29.557057Z","shell.execute_reply.started":"2025-10-04T05:59:29.551308Z","shell.execute_reply":"2025-10-04T05:59:29.556332Z"}},"outputs":[],"execution_count":10},{"id":"e9964e11-9bee-4816-9605-2995e455e6ee","cell_type":"code","source":"def save_ensemble(ensemble, filepath, method='joblib'):\n    \"\"\"Save ensemble model to disk\"\"\"\n    Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n    \n    # Ensure all required attributes exist\n    if not hasattr(ensemble, 'feature_names'):\n        ensemble.feature_names = []\n    if not hasattr(ensemble, 'training_params'):\n        ensemble.training_params = {}\n    if not hasattr(ensemble, 'target_cols'):\n        ensemble.target_cols = [f\"target_{i}\" for i in range(424)]\n    if not hasattr(ensemble, 'use_gpu'):\n        ensemble.use_gpu = True\n    \n    if method == 'joblib':\n        joblib.dump(ensemble, filepath)\n        print(f\"Ensemble saved to {filepath}\")\n    else:\n        with open(filepath, 'wb') as f:\n            pickle.dump(ensemble, f)\n        print(f\"Ensemble saved to {filepath}\")\n\ndef load_ensemble(filepath):\n    \"\"\"Load ensemble model from disk\"\"\"\n    if filepath.endswith('.joblib'):\n        ensemble = joblib.load(filepath)\n    else:\n        with open(filepath, 'rb') as f:\n            ensemble = pickle.load(f)\n    print(f\"Ensemble loaded from {filepath}\")\n    return ensemble\n\ndef save_models_list(models_dict, filepath):\n    \"\"\"Save models as a list for your original ensemble_predict function\"\"\"\n    all_models = []\n    for target_models in models_dict.values():\n        all_models.extend(list(target_models.values()))\n    \n    joblib.dump(all_models, filepath)\n    print(f\"Models list saved to {filepath} with {len(all_models)} models\")\n\ndef load_models_list(filepath):\n    \"\"\"Load models list for your original ensemble_predict function\"\"\"\n    models = joblib.load(filepath)\n    print(f\"Models list loaded from {filepath} with {len(models)} models\")\n    return models\n\n# Save the ensemble\n# save_ensemble(ensemble, '/kaggle/working/my_ensemble.joblib')\n# save_models_list(ensemble.models, '/kaggle/working/models_list.joblib')\n\nprint(\"Save/load functions ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T05:59:59.791944Z","iopub.execute_input":"2025-10-04T05:59:59.792217Z","iopub.status.idle":"2025-10-04T05:59:59.808309Z","shell.execute_reply.started":"2025-10-04T05:59:59.792193Z","shell.execute_reply":"2025-10-04T05:59:59.807654Z"}},"outputs":[{"name":"stdout","text":"Save/load functions ready!\n","output_type":"stream"}],"execution_count":12},{"id":"67b4c687-d8eb-494b-a56e-4c57c3218535","cell_type":"code","source":"import polars as pl\nimport pandas as pd\nimport numpy as np\n\ndef predict_test(\n    test: pl.DataFrame,\n    lag1: pl.DataFrame, \n    lag2: pl.DataFrame,\n    lag3: pl.DataFrame,\n    lag4: pl.DataFrame,\n) -> pl.DataFrame:\n    \"\"\"\n    Predicts target values using lag features.\n    Handles both Polars and Pandas DataFrame inputs.\n    \"\"\"\n    # Convert all inputs to Pandas if they are Polars DataFrames\n    if isinstance(test, pl.DataFrame):\n        test_pd = test.to_pandas()\n    else:\n        test_pd = test.copy()\n        \n    if isinstance(lag1, pl.DataFrame):\n        lag1_pd = lag1.to_pandas()\n    else:\n        lag1_pd = lag1.copy()\n        \n    if isinstance(lag2, pl.DataFrame):\n        lag2_pd = lag2.to_pandas()\n    else:\n        lag2_pd = lag2.copy()\n        \n    if isinstance(lag3, pl.DataFrame):\n        lag3_pd = lag3.to_pandas()\n    else:\n        lag3_pd = lag3.copy()\n        \n    if isinstance(lag4, pl.DataFrame):\n        lag4_pd = lag4.to_pandas()\n    else:\n        lag4_pd = lag4.copy()\n    \n    print(f\"Input shapes - test: {test_pd.shape}, lag1: {lag1_pd.shape}, lag2: {lag2_pd.shape}, lag3: {lag3_pd.shape}, lag4: {lag4_pd.shape}\")\n    \n    # Load models\n    try:\n        Models = load_models_list('/kaggle/input/mitsuienslearning/models_list.joblib')\n        print(f\"Loaded {len(Models)} models for prediction\")\n    except:\n        # Fallback: load ensemble and extract models\n        ensemble = load_ensemble('/kaggle/working/my_ensemble.joblib')\n        Models = []\n        for target_models in ensemble.models.values():\n            Models.extend(list(target_models.values()))\n        print(f\"Loaded {len(Models)} models from ensemble\")\n    \n    # Combine lag features in Pandas\n    X_pred = pd.concat([\n        test_pd[[\"date_id\"]],\n        lag1_pd[target_lag_1],\n        lag2_pd[target_lag_2], \n        lag3_pd[target_lag_3],\n        lag4_pd[target_lag_4],\n    ], axis=1)\n    \n    # If no rows, return all zeros as Polars DataFrame\n    if len(X_pred) == 0:\n        zero_df = pl.DataFrame({f\"target_{i}\": [0.0] for i in range(424)})\n        print(\"Empty input, returning zeros\")\n        return zero_df\n    \n    # Fill nulls with 0\n    X_pred = X_pred.fillna(0)\n    \n    # Prepare features for prediction\n    n_targets = 424\n    n_rows = X_pred.shape[0]\n    \n    print(f\"Creating features for {n_rows} rows and {n_targets} targets...\")\n    \n    # Create features for all targets efficiently\n    features_array = np.tile(X_pred[Features[1:]].values, (n_targets, 1))\n    target_ids = np.repeat(np.arange(n_targets), n_rows)\n    \n    # Create prediction DataFrame\n    X_pred2 = pd.DataFrame({\n        \"date_id\": np.tile(X_pred[\"date_id\"].values, n_targets),\n        **{feat: features_array[:, i] for i, feat in enumerate(Features[1:])},\n        \"target_id\": target_ids,\n        \"row\": np.tile(np.arange(n_rows), n_targets)\n    })\n    \n    print(f\"Prediction DataFrame shape: {X_pred2.shape}\")\n    \n    # Make predictions\n    preds = ensemble_predict_from_list(Models, X_pred2[Features2])\n    X_pred2 = X_pred2.assign(preds=preds)\n    \n    print(f\"Predictions completed, min: {preds.min():.4f}, max: {preds.max():.4f}, mean: {preds.mean():.4f}\")\n    \n    # Pivot to wide format using Pandas\n    df_wide = (\n        X_pred2.groupby([\"target_id\", \"row\"])\n        .agg({\"preds\": \"first\"})\n        .reset_index()\n        .pivot(index=\"row\", columns=\"target_id\", values=\"preds\")\n        .sort_index()\n    )\n    \n    # Ensure correct column order and naming\n    df_wide = df_wide.reindex(columns=range(424), fill_value=0)\n    df_wide.columns = [f\"target_{i}\" for i in range(424)]\n    \n    print(f\"Wide format shape: {df_wide.shape}\")\n    \n    # Return last row as predictions and convert to Polars DataFrame\n    result_df = df_wide.tail(1)\n    result_pl = pl.from_pandas(result_df.reset_index(drop=True))\n    \n    print(f\"Final result shape: {result_pl.shape}\")\n    return result_pl\n\n# Alternative version that's more robust for competition\ndef predict_robust(\n    test: pl.DataFrame,\n    lag1: pl.DataFrame, \n    lag2: pl.DataFrame,\n    lag3: pl.DataFrame,\n    lag4: pl.DataFrame,\n) -> pl.DataFrame:\n    \"\"\"\n    More robust prediction function with better error handling.\n    \"\"\"\n    try:\n        return predict(test, lag1, lag2, lag3, lag4)\n    except Exception as e:\n        print(f\"Prediction error: {e}\")\n        # Return default zeros in case of error\n        return pl.DataFrame({f\"target_{i}\": [0.0] for i in range(424)})\n\n# Test function with sample Polars data\ndef test_polars_prediction():\n    \"\"\"Test the prediction function with Polars DataFrames\"\"\"\n    print(\"=== TESTING POLARS PREDICTION ===\")\n    \n    # Convert sample data to Polars\n    sample_test_pl = pl.from_pandas(trainl[Features].iloc[:5])\n    sample_lag1_pl = pl.from_pandas(trainl[target_lag_1].iloc[:5])\n    sample_lag2_pl = pl.from_pandas(trainl[target_lag_2].iloc[:5])\n    sample_lag3_pl = pl.from_pandas(trainl[target_lag_3].iloc[:5])\n    sample_lag4_pl = pl.from_pandas(trainl[target_lag_4].iloc[:5])\n    \n    print(\"Input types:\")\n    print(f\"test: {type(sample_test_pl)}\")\n    print(f\"lag1: {type(sample_lag1_pl)}\")\n    print(f\"lag2: {type(sample_lag2_pl)}\")\n    print(f\"lag3: {type(sample_lag3_pl)}\")\n    print(f\"lag4: {type(sample_lag4_pl)}\")\n    \n    # Test prediction\n    result = predict_test(sample_test_pl, sample_lag1_pl, sample_lag2_pl, sample_lag3_pl, sample_lag4_pl)\n    \n    print(f\"Result type: {type(result)}\")\n    print(f\"Result shape: {result.shape}\")\n    print(f\"Result columns: {result.columns[:10]}...\")\n    \n    return result\n\n# Run the test\ntest_result = test_polars_prediction()\n\n# Final competition-ready predict function\ndef predict(\n    test: pl.DataFrame,\n    lag1: pl.DataFrame, \n    lag2: pl.DataFrame,\n    lag3: pl.DataFrame,\n    lag4: pl.DataFrame,\n) -> pl.DataFrame:\n    \"\"\"\n    Competition-ready prediction function with comprehensive error handling.\n    \"\"\"\n    print(\"=== STARTING PREDICTION ===\")\n    \n    try:\n        # Convert all inputs to pandas\n        test_pd = test.to_pandas() if isinstance(test, pl.DataFrame) else test\n        lag1_pd = lag1.to_pandas() if isinstance(lag1, pl.DataFrame) else lag1\n        lag2_pd = lag2.to_pandas() if isinstance(lag2, pl.DataFrame) else lag2  \n        lag3_pd = lag3.to_pandas() if isinstance(lag3, pl.DataFrame) else lag3\n        lag4_pd = lag4.to_pandas() if isinstance(lag4, pl.DataFrame) else lag4\n        \n        # Load models\n        Models = load_models_list('/kaggle/input/mitsuienslearning/models_list.joblib')\n        \n        # Create feature matrix\n        X_pred = pd.concat([\n            test_pd[[\"date_id\"]],\n            lag1_pd[target_lag_1],\n            lag2_pd[target_lag_2],\n            lag3_pd[target_lag_3], \n            lag4_pd[target_lag_4],\n        ], axis=1).fillna(0)\n        \n        if len(X_pred) == 0:\n            return pl.DataFrame({f\"target_{i}\": [0.0] for i in range(424)})\n        \n        # Create prediction matrix for all targets\n        n_rows = len(X_pred)\n        features_array = np.tile(X_pred[Features[1:]].values, (424, 1))\n        target_ids = np.repeat(np.arange(424), n_rows)\n        \n        X_pred2 = pd.DataFrame({\n            \"date_id\": np.tile(X_pred[\"date_id\"].values, 424),\n            **{feat: features_array[:, i] for i, feat in enumerate(Features[1:])},\n            \"target_id\": target_ids,\n            \"row\": np.tile(np.arange(n_rows), 424)\n        })\n        \n        # Make predictions\n        preds = ensemble_predict_from_list(Models, X_pred2[Features2])\n        X_pred2['preds'] = preds\n        \n        # Convert to wide format\n        df_wide = (X_pred2.pivot_table(index='row', columns='target_id', values='preds', aggfunc='first')\n                  .reindex(columns=range(424), fill_value=0)\n                  .rename(columns=lambda x: f\"target_{x}\"))\n        \n        # Return last row as Polars DataFrame\n        result = pl.from_pandas(df_wide.tail(1).reset_index(drop=True))\n        print(\"=== PREDICTION COMPLETED SUCCESSFULLY ===\")\n        return result\n        \n    except Exception as e:\n        print(f\"Prediction failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        # Return safe default\n        return pl.DataFrame({f\"target_{i}\": [0.0] for i in range(424)})\n\nprint(\"Prediction block ready! Use 'competition_predict' for submission.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T05:59:37.051940Z","iopub.execute_input":"2025-10-04T05:59:37.052493Z","iopub.status.idle":"2025-10-04T05:59:53.757280Z","shell.execute_reply.started":"2025-10-04T05:59:37.052472Z","shell.execute_reply":"2025-10-04T05:59:53.756591Z"}},"outputs":[{"name":"stdout","text":"=== TESTING POLARS PREDICTION ===\nInput types:\ntest: <class 'polars.dataframe.frame.DataFrame'>\nlag1: <class 'polars.dataframe.frame.DataFrame'>\nlag2: <class 'polars.dataframe.frame.DataFrame'>\nlag3: <class 'polars.dataframe.frame.DataFrame'>\nlag4: <class 'polars.dataframe.frame.DataFrame'>\nInput shapes - test: (5, 425), lag1: (5, 106), lag2: (5, 106), lag3: (5, 106), lag4: (5, 106)\nModels list loaded from /kaggle/input/mitsuienslearning/models_list.joblib with 1272 models\nLoaded 1272 models for prediction\nCreating features for 5 rows and 424 targets...\nPrediction DataFrame shape: (2120, 427)\nPredictions completed, min: -0.0002, max: 0.0000, mean: -0.0002\nWide format shape: (5, 424)\nFinal result shape: (1, 424)\nResult type: <class 'polars.dataframe.frame.DataFrame'>\nResult shape: (1, 424)\nResult columns: ['target_0', 'target_1', 'target_2', 'target_3', 'target_4', 'target_5', 'target_6', 'target_7', 'target_8', 'target_9']...\nPrediction block ready! Use 'competition_predict' for submission.\n","output_type":"stream"}],"execution_count":11},{"id":"62864c5f-02da-4faf-8a47-87c717d07185","cell_type":"markdown","source":"import numpy as np\nimport pandas as pd\n\ndef rank_correlation_sharpe_ratio(merged_df: pd.DataFrame) -> float:\n    \"\"\"\n    Calculate the rank correlation Sharpe ratio for the competition metric.\n    Improved with better error handling for zero denominators.\n    \"\"\"\n    prediction_cols = [col for col in merged_df.columns if col.startswith('prediction_')]\n    target_cols = [col for col in merged_df.columns if col.startswith('target_')]\n    \n    def _compute_rank_correlation(row):\n        try:\n            non_null_targets = [col for col in target_cols if not pd.isnull(row[col])]\n            matching_predictions = [col for col in prediction_cols if col.replace('prediction', 'target') in non_null_targets]\n            \n            if len(non_null_targets) < 2:  # Need at least 2 values for correlation\n                return np.nan\n            \n            target_vals = row[non_null_targets].values\n            pred_vals = row[matching_predictions].values\n            \n            # Check for zero variance\n            if np.std(target_vals, ddof=0) == 0 or np.std(pred_vals, ddof=0) == 0:\n                return np.nan\n            \n            # Calculate rank correlation\n            target_ranks = pd.Series(target_vals).rank(method='average')\n            pred_ranks = pd.Series(pred_vals).rank(method='average')\n            \n            correlation = np.corrcoef(pred_ranks, target_ranks)[0, 1]\n            return correlation\n            \n        except Exception as e:\n            return np.nan\n    \n    # Calculate daily rank correlations, ignoring NaN values\n    daily_rank_corrs = merged_df.apply(_compute_rank_correlation, axis=1)\n    valid_corrs = daily_rank_corrs.dropna()\n    \n    if len(valid_corrs) == 0:\n        return 0.0  # Return 0 instead of error if no valid correlations\n    \n    std_dev = valid_corrs.std(ddof=0)\n    if std_dev == 0:\n        return 0.0  # Return 0 instead of error if zero standard deviation\n    \n    sharpe_ratio = valid_corrs.mean() / std_dev\n    return float(sharpe_ratio)\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame) -> float:\n    \"\"\"\n    Competition scoring function with improved error handling.\n    \"\"\"\n    try:\n        # Ensure both dataframes have the same columns\n        assert all(solution.columns == submission.columns)\n        \n        # Create copies to avoid modifying originals\n        submission_copy = submission.copy()\n        solution_copy = solution.copy()\n        \n        # Rename columns for the scoring function\n        submission_copy = submission_copy.rename(columns={\n            col: col.replace('target_', 'prediction_') for col in submission_copy.columns\n        })\n        \n        # Replace zeros with None as in original\n        solution_copy = solution_copy.replace(0, None)\n        \n        # Merge and calculate score\n        merged_df = pd.concat([solution_copy, submission_copy], axis=1)\n        return rank_correlation_sharpe_ratio(merged_df)\n        \n    except Exception as e:\n        print(f\"Error in score function: {e}\")\n        return 0.0  # Return 0 instead of -1 for errors\n\ndef robust_score(solution: pd.DataFrame, submission: pd.DataFrame, verbose=True) -> float:\n    \"\"\"\n    Robust version of the scoring function with comprehensive error handling.\n    \"\"\"\n    try:\n        if verbose:\n            print(f\"Input shapes - Solution: {solution.shape}, Submission: {submission.shape}\")\n        \n        # Ensure both dataframes have exactly 424 target columns\n        required_cols = [f'target_{i}' for i in range(424)]\n        \n        # Add missing columns with zeros\n        for col in required_cols:\n            if col not in submission.columns:\n                submission[col] = 0.0\n            if col not in solution.columns:\n                solution[col] = 0.0\n        \n        # Reorder columns to ensure consistency\n        submission = submission[required_cols]\n        solution = solution[required_cols]\n        \n        if verbose:\n            print(f\"After alignment - Solution: {solution.shape}, Submission: {submission.shape}\")\n            print(f\"Sample data - Solution mean: {solution.mean().mean():.4f}, Submission mean: {submission.mean().mean():.4f}\")\n        \n        # Remove rows where all targets are zero or NaN (no information)\n        solution_non_zero = solution.replace(0, np.nan).dropna(how='all')\n        if len(solution_non_zero) == 0:\n            if verbose:\n                print(\"Warning: All solution values are zero\")\n            return 0.0\n        \n        # Use only rows that have non-zero solutions\n        valid_indices = solution_non_zero.index\n        solution_filtered = solution.loc[valid_indices]\n        submission_filtered = submission.loc[valid_indices]\n        \n        if verbose:\n            print(f\"After filtering zeros - Solution: {solution_filtered.shape}, Submission: {submission_filtered.shape}\")\n        \n        if len(solution_filtered) == 0:\n            if verbose:\n                print(\"Warning: No valid rows after filtering\")\n            return 0.0\n        \n        # Calculate score\n        score_value = score(solution_filtered, submission_filtered)\n        \n        if verbose:\n            print(f\"Final score: {score_value:.6f}\")\n        \n        return score_value\n        \n    except Exception as e:\n        print(f\"Error in robust_score: {e}\")\n        return 0.0\n\ndef debug_data_issues(solution: pd.DataFrame, submission: pd.DataFrame):\n    \"\"\"\n    Debug function to identify data issues that might cause scoring problems.\n    \"\"\"\n    print(\"=== DATA DEBUG INFO ===\")\n    print(f\"Solution shape: {solution.shape}\")\n    print(f\"Submission shape: {submission.shape}\")\n    print(f\"Solution columns: {len(solution.columns)}\")\n    print(f\"Submission columns: {len(submission.columns)}\")\n    \n    # Check for all zeros\n    solution_zeros = (solution == 0).all(axis=1).sum()\n    submission_zeros = (submission == 0).all(axis=1).sum()\n    print(f\"Rows with all zeros - Solution: {solution_zeros}, Submission: {submission_zeros}\")\n    \n    # Check for NaN values\n    solution_nans = solution.isna().sum().sum()\n    submission_nans = submission.isna().sum().sum()\n    print(f\"NaN values - Solution: {solution_nans}, Submission: {submission_nans}\")\n    \n    # Check variance\n    solution_var = solution.var(axis=1)\n    submission_var = submission.var(axis=1)\n    print(f\"Zero variance rows - Solution: {(solution_var == 0).sum()}, Submission: {(submission_var == 0).sum()}\")\n    \n    # Check data ranges\n    print(f\"Solution range: [{solution.min().min():.4f}, {solution.max().max():.4f}]\")\n    print(f\"Submission range: [{submission.min().min():.4f}, {submission.max().max():.4f}]\")\n\n# Test the scoring with your predictions\ndef test_scoring_with_predictions():\n    \"\"\"Test scoring with the generated predictions\"\"\"\n    print(\"=== TESTING SCORING ===\")\n    \n    # Make predictions\n    X_data = X_train.copy()\n    X_data[\"preds\"] = ensemble_predict(ensemble.models, X_train)\n    \n    # Convert to wide format\n    df_preds = X_data.copy()\n    df_preds['row'] = df_preds.groupby('target_id').cumcount()\n    df_wide = df_preds.pivot(index='row', columns='target_id', values='preds')\n    df_wide = df_wide.sort_index(axis=1)\n    df_wide.columns = [f'target_{int(col)}' for col in df_wide.columns]\n    \n    # Ensure all 424 targets\n    for i in range(424):\n        col_name = f'target_{i}'\n        if col_name not in df_wide.columns:\n            df_wide[col_name] = 0.0\n    \n    target_cols = [f'target_{i}' for i in range(424)]\n    df_wide = df_wide[target_cols]\n    \n    print(f\"Prediction shape: {df_wide.shape}\")\n    \n    # Prepare solution data\n    solution_template = trainl.copy().set_index('date_id')\n    solution_template = solution_template.rename(columns={\n        col: f\"target_{int(col.split('_')[1])}\" if col.startswith('target_') else col \n        for col in solution_template.columns\n    })\n    \n    # Use first 90 rows for scoring (matching competition format)\n    solution_subset = solution_template.iloc[:90][target_cols].reset_index(drop=True)\n    prediction_subset = df_wide.iloc[:90].reset_index(drop=True)\n    \n    # Debug data issues\n    debug_data_issues(solution_subset, prediction_subset)\n    \n    # Calculate score with robust function\n    score_value = robust_score(solution_subset, prediction_subset, verbose=True)\n    print(f\"FINAL SCORE: {score_value:.6f}\")\n    return score_value\n\n# Quick validation score function\ndef calculate_validation_score(ensemble, validation_size=0.1):\n    \"\"\"\n    Calculate validation score on a holdout set.\n    \"\"\"\n    print(\"\\n=== CALCULATING VALIDATION SCORE ===\")\n    \n    # Create validation set (last portion of data)\n    n_samples = len(training_df)\n    val_start = int(n_samples * (1 - validation_size))\n    \n    X_val = training_df[Features2].iloc[val_start:]\n    y_val = training_df[\"target\"].iloc[val_start:]\n    target_ids_val = training_df[\"target_id\"].iloc[val_start:]\n    \n    print(f\"Validation set size: {len(X_val)}\")\n    \n    # Make predictions\n    val_preds = ensemble_predict(ensemble.models, X_val)\n    \n    # Convert to wide format\n    val_data = X_val.copy()\n    val_data['preds'] = val_preds\n    val_data['row'] = val_data.groupby('target_id').cumcount()\n    \n    df_val_wide = val_data.pivot(index='row', columns='target_id', values='preds')\n    df_val_wide = df_val_wide.sort_index(axis=1)\n    df_val_wide.columns = [f'target_{int(col)}' for col in df_val_wide.columns]\n    \n    # Ensure all targets\n    for i in range(424):\n        col_name = f'target_{i}'\n        if col_name not in df_val_wide.columns:\n            df_val_wide[col_name] = 0.0\n    \n    target_cols = [f'target_{i}' for i in range(424)]\n    df_val_wide = df_val_wide[target_cols]\n    \n    # Prepare solution data for validation period\n    solution_template = trainl.copy().set_index('date_id')\n    solution_template = solution_template.rename(columns={\n        col: f\"target_{int(col.split('_')[1])}\" if col.startswith('target_') else col \n        for col in solution_template.columns\n    })\n    \n    # Use corresponding rows for validation scoring\n    val_solution = solution_template.iloc[-len(df_val_wide):][target_cols].reset_index(drop=True)\n    val_predictions = df_val_wide.reset_index(drop=True)\n    \n    # Debug validation data\n    debug_data_issues(val_solution, val_predictions)\n    \n    # Calculate validation score\n    val_score = robust_score(val_solution, val_predictions, verbose=True)\n    print(f\"VALIDATION SCORE: {val_score:.6f}\")\n    \n    return val_score\n\n# Run scoring tests\nprint(\"Running comprehensive scoring tests...\")\nfinal_score = test_scoring_with_predictions()\nvalidation_score = calculate_validation_score(ensemble)\n\nprint(f\"\\n=== FINAL SUMMARY ===\")\nprint(f\"Training completed for {len(ensemble.models)} targets\")\nprint(f\"Test score: {final_score:.6f}\")\nprint(f\"Validation score: {validation_score:.6f}\")\n\n# Additional diagnostic: Check if we need more targets trained\nif len(ensemble.models) < 10:\n    print(f\"\\n⚠️  WARNING: Only {len(ensemble.models)} targets trained. Consider training more targets for better scores.\")\nelse:\n    print(f\"✓ Good: {len(ensemble.models)} targets trained\")","metadata":{"execution":{"iopub.status.busy":"2025-10-04T06:00:06.947583Z","iopub.execute_input":"2025-10-04T06:00:06.947885Z","iopub.status.idle":"2025-10-04T06:00:07.938767Z","shell.execute_reply.started":"2025-10-04T06:00:06.947866Z","shell.execute_reply":"2025-10-04T06:00:07.937864Z"}}},{"id":"48959ca6","cell_type":"code","source":"# submission through the API\nimport os\nimport kaggle_evaluation.mitsui_inference_server\n\ninference_server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print('there')\n    # inference_server.serve()\nelse:\n    print('here')\n    inference_server.run_local_gateway(('/kaggle/input/mitsui-commodity-prediction-challenge/',))","metadata":{"papermill":{"duration":49.005971,"end_time":"2025-09-08T12:22:13.642777","exception":false,"start_time":"2025-09-08T12:21:24.636806","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e9a1959c","cell_type":"code","source":"display(pl.read_parquet('/kaggle/working/submission.parquet'))","metadata":{"papermill":{"duration":0.141377,"end_time":"2025-09-08T12:22:13.791255","exception":false,"start_time":"2025-09-08T12:22:13.649878","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T06:26:32.239436Z","iopub.execute_input":"2025-10-04T06:26:32.239934Z","iopub.status.idle":"2025-10-04T06:26:32.292814Z","shell.execute_reply.started":"2025-10-04T06:26:32.239909Z","shell.execute_reply":"2025-10-04T06:26:32.292178Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"shape: (134, 425)\n┌─────────┬───────────┬───────────┬───────────┬───┬────────────┬───────────┬───────────┬───────────┐\n│ date_id ┆ target_0  ┆ target_1  ┆ target_2  ┆ … ┆ target_420 ┆ target_42 ┆ target_42 ┆ target_42 │\n│ ---     ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---        ┆ 1         ┆ 2         ┆ 3         │\n│ i64     ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64        ┆ ---       ┆ ---       ┆ ---       │\n│         ┆           ┆           ┆           ┆   ┆            ┆ f64       ┆ f64       ┆ f64       │\n╞═════════╪═══════════╪═══════════╪═══════════╪═══╪════════════╪═══════════╪═══════════╪═══════════╡\n│ 1827    ┆ -0.000068 ┆ -0.000068 ┆ -0.000068 ┆ … ┆ -0.000068  ┆ -0.000068 ┆ -0.000068 ┆ -0.000068 │\n│ 1828    ┆ -0.000068 ┆ -0.000068 ┆ -0.000068 ┆ … ┆ -0.000068  ┆ -0.000068 ┆ -0.000068 ┆ -0.000068 │\n│ 1829    ┆ -0.000136 ┆ -0.000136 ┆ -0.000136 ┆ … ┆ -0.000136  ┆ -0.000136 ┆ -0.000136 ┆ -0.000136 │\n│ 1830    ┆ -0.000235 ┆ -0.000235 ┆ -0.000235 ┆ … ┆ -0.000235  ┆ -0.000235 ┆ -0.000235 ┆ -0.000235 │\n│ 1831    ┆ -0.000026 ┆ -0.000026 ┆ -0.000026 ┆ … ┆ -0.000026  ┆ -0.000026 ┆ -0.000026 ┆ -0.000026 │\n│ …       ┆ …         ┆ …         ┆ …         ┆ … ┆ …          ┆ …         ┆ …         ┆ …         │\n│ 1956    ┆ 0.000014  ┆ 0.000014  ┆ 0.000014  ┆ … ┆ 0.000014   ┆ 0.000014  ┆ 0.000014  ┆ 0.000014  │\n│ 1957    ┆ -0.000012 ┆ -0.000012 ┆ -0.000012 ┆ … ┆ -0.000012  ┆ -0.000012 ┆ -0.000012 ┆ -0.000012 │\n│ 1958    ┆ 0.000016  ┆ 0.000016  ┆ 0.000016  ┆ … ┆ 0.000016   ┆ 0.000016  ┆ 0.000016  ┆ 0.000016  │\n│ 1959    ┆ 0.000016  ┆ 0.000016  ┆ 0.000016  ┆ … ┆ 0.000016   ┆ 0.000016  ┆ 0.000016  ┆ 0.000016  │\n│ 1960    ┆ 0.000035  ┆ 0.000035  ┆ 0.000035  ┆ … ┆ 0.000035   ┆ 0.000035  ┆ 0.000035  ┆ 0.000035  │\n└─────────┴───────────┴───────────┴───────────┴───┴────────────┴───────────┴───────────┴───────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (134, 425)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>target_0</th><th>target_1</th><th>target_2</th><th>target_3</th><th>target_4</th><th>target_5</th><th>target_6</th><th>target_7</th><th>target_8</th><th>target_9</th><th>target_10</th><th>target_11</th><th>target_12</th><th>target_13</th><th>target_14</th><th>target_15</th><th>target_16</th><th>target_17</th><th>target_18</th><th>target_19</th><th>target_20</th><th>target_21</th><th>target_22</th><th>target_23</th><th>target_24</th><th>target_25</th><th>target_26</th><th>target_27</th><th>target_28</th><th>target_29</th><th>target_30</th><th>target_31</th><th>target_32</th><th>target_33</th><th>target_34</th><th>target_35</th><th>&hellip;</th><th>target_387</th><th>target_388</th><th>target_389</th><th>target_390</th><th>target_391</th><th>target_392</th><th>target_393</th><th>target_394</th><th>target_395</th><th>target_396</th><th>target_397</th><th>target_398</th><th>target_399</th><th>target_400</th><th>target_401</th><th>target_402</th><th>target_403</th><th>target_404</th><th>target_405</th><th>target_406</th><th>target_407</th><th>target_408</th><th>target_409</th><th>target_410</th><th>target_411</th><th>target_412</th><th>target_413</th><th>target_414</th><th>target_415</th><th>target_416</th><th>target_417</th><th>target_418</th><th>target_419</th><th>target_420</th><th>target_421</th><th>target_422</th><th>target_423</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1827</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>&hellip;</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td></tr><tr><td>1828</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>&hellip;</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td><td>-0.000068</td></tr><tr><td>1829</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>&hellip;</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td><td>-0.000136</td></tr><tr><td>1830</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>&hellip;</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td><td>-0.000235</td></tr><tr><td>1831</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>&hellip;</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td><td>-0.000026</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1956</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>&hellip;</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td><td>0.000014</td></tr><tr><td>1957</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>&hellip;</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td><td>-0.000012</td></tr><tr><td>1958</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>&hellip;</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td></tr><tr><td>1959</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>&hellip;</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td><td>0.000016</td></tr><tr><td>1960</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>&hellip;</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td><td>0.000035</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":16},{"id":"92645eab","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.00639,"end_time":"2025-09-08T12:22:13.818388","exception":false,"start_time":"2025-09-08T12:22:13.811998","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}