{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94771,"databundleVersionId":13613251,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13256549,"sourceType":"datasetVersion","datasetId":8400276}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":765.21753,"end_time":"2025-09-08T12:22:14.944658","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-08T12:09:29.727128","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bade4ff9","cell_type":"markdown","source":"---\n\n**In this notebook, we will use Ensembling Gradient Boosting trees for a multi-target regression problem, leveraging lagged targets to predict 424 outputs. To speed up inference, we adopt the long-format multi-output prediction method, which is much faster than the standard multi-output approach.**\n\n**You will also find several useful techniques in this notebook, including:**\n\n* How to create lagged targets and use them in the prediction step\n* How to run Ensembling Gradient Boosting trees with lags targets\n* How to build an optimized prediction function for API inference\n\n\n\n---","metadata":{"papermill":{"duration":0.00386,"end_time":"2025-09-08T12:09:35.266874","exception":false,"start_time":"2025-09-08T12:09:35.263014","status":"completed"},"tags":[]}},{"id":"c65a313c","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Data loading\np = '/kaggle/input/mitsui-commodity-prediction-challenge/'\ntrain = pd.read_csv(p+'train.csv')\ntrainl = pd.read_csv(p+'train_labels.csv')\ntraint = pd.read_csv(p+'target_pairs.csv')\n\ndef _handle_missing_values(data):\n    data.interpolate(method='polynomial', order=3, inplace=True)\n    data.clip(lower=-10, upper=10, inplace=True)\n    return data\n\ntrain = _handle_missing_values(train)\ntrainl = _handle_missing_values(trainl)\n\ntarget_lag_1 = traint.loc[traint[\"lag\"]==1,\"target\"].values\ntarget_lag_2 = traint.loc[traint[\"lag\"]==2,\"target\"].values\ntarget_lag_3 = traint.loc[traint[\"lag\"]==3,\"target\"].values\ntarget_lag_4 = traint.loc[traint[\"lag\"]==4,\"target\"].values\n\nFeatures = [i for i in trainl.columns]\n\ndef create_lagged_labels(df):\n    dt = pd.DataFrame()\n    dt[\"date_id\"] = df[\"date_id\"]\n    for f in Features[1:]:\n        if f in target_lag_1:\n            lag = 1\n        elif f in target_lag_2:\n            lag = 2\n        elif f in target_lag_3:\n            lag = 3\n        elif f in target_lag_4:\n            lag = 4    \n        dt[f] = df[f].shift(lag).fillna(0)\n    return df, dt\n\n_, train_lagged = create_lagged_labels(trainl)\n\n# Create training data\nimport gc\ntraining_df = []\ntarget_cols = [f\"target_{i}\" for i in range(424)]\nfor j, target_col in enumerate(target_cols):\n    temp_train_df = pd.DataFrame()\n    temp_train_df[Features] = train_lagged[Features]                     \n    temp_train_df['target_id'] = j\n    y = trainl[target_col].values\n    temp_train_df['target'] = y\n    mask = ~(np.isnan(y) | np.isinf(y) | (np.abs(y) > 1e10))\n    training_df.append(temp_train_df[mask].copy())\n    del temp_train_df, y\n    gc.collect()\n\ntraining_df = pd.concat(training_df).reset_index(drop=True)\nFeatures2 = Features + [\"target_id\"]\nX_train = training_df[Features2]\ny_train = training_df[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2025-10-04T08:06:34.692424Z","iopub.execute_input":"2025-10-04T08:06:34.693401Z","iopub.status.idle":"2025-10-04T08:08:25.936100Z","shell.execute_reply.started":"2025-10-04T08:06:34.693370Z","shell.execute_reply":"2025-10-04T08:08:25.935454Z"},"papermill":{"duration":0.010266,"end_time":"2025-09-08T12:09:42.114913","exception":false,"start_time":"2025-09-08T12:09:42.104647","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"id":"7f5373ca-44c7-4af2-a8f8-e582269338ab","cell_type":"code","source":"# Deeper Gradient Boosting models for regression\nxgb_model = XGBRegressor(\n    n_estimators=2000,\n    max_depth=6,\n    learning_rate=0.01,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=1,\n    reg_lambda=1,\n    random_state=42,\n    tree_method=\"hist\",\n    device=\"cuda\"\n)\n\nlgbm_model = LGBMRegressor(\n    n_estimators=2000,\n    max_depth=6,\n    learning_rate=0.01,\n    num_leaves=256,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=1,\n    reg_lambda=1,\n    random_state=42,\n    device=\"gpu\",\n    verbose=-1\n)\n\ncatboost_model = CatBoostRegressor(\n    iterations=2000,\n    depth=6,\n    learning_rate=0.01,\n    l2_leaf_reg=3,\n    random_seed=42,\n    loss_function='RMSE',\n    task_type=\"GPU\",\n    verbose=False\n)\n\n# Append models to a list for later training / ensembling\nmodels = [xgb_model, lgbm_model, catboost_model]\nModels = []\n\n# Train all models on the entire dataset (no target-specific training)\nprint(\"Training models on entire dataset...\")\nfor model in models:\n    model.fit(X_train, y_train)\n    Models.append(model)\n\nprint(f\"Models list created with {len(Models)} models.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:08:25.937432Z","iopub.execute_input":"2025-10-04T08:08:25.937852Z","iopub.status.idle":"2025-10-04T08:14:36.045751Z","shell.execute_reply.started":"2025-10-04T08:08:25.937825Z","shell.execute_reply":"2025-10-04T08:14:36.045091Z"}},"outputs":[{"name":"stdout","text":"Training models on entire dataset...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"Models list created with 3 models.\n","output_type":"stream"}],"execution_count":2},{"id":"c7b8642b-66c9-47f8-a3f5-405d79bbdfdd","cell_type":"code","source":"def ensemble_predict(models, X):\n    \"\"\"\n    Predict using a list of trained models and return the averaged prediction.\n    \n    Parameters:\n        models : list of trained models\n        X      : numpy array or DataFrame of features\n        \n    Returns:\n        ensemble_pred : averaged prediction across models\n    \"\"\"\n    preds = [model.predict(X) for model in models]\n    ensemble_pred = np.mean(preds, axis=0)\n    return ensemble_pred\n\n# Test the predictions\nX_data = X_train.copy()\nX_data[\"preds\"] = ensemble_predict(Models, X_train)\n\n# Convert to wide format (90 rows × 424 columns)\ndf_preds = X_data.copy()\ndf_preds['row'] = df_preds.groupby('target_id').cumcount()\n\n# Pivot the table to wide format\ndf_wide = df_preds.pivot(index='row', columns='target_id', values='preds')\ndf_wide = df_wide.sort_index(axis=1)\ndf_wide.index = [i for i in df_wide.index]\n\n# Rename columns\ndf_wide.columns = [f'target_{i}' for i in df_wide.columns]\nprint(f\"Wide format shape: {df_wide.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:14:36.046315Z","iopub.execute_input":"2025-10-04T08:14:36.046530Z"}},"outputs":[],"execution_count":null},{"id":"d4c4a2dd-0b41-4e93-839b-bb2c91059e59","cell_type":"code","source":"def rank_correlation_sharpe_ratio(merged_df: pd.DataFrame) -> float:\n    prediction_cols = [col for col in merged_df.columns if col.startswith('prediction_')]\n    target_cols = [col for col in merged_df.columns if col.startswith('target_')]\n    \n    def _compute_rank_correlation(row):\n        non_null_targets = [col for col in target_cols if not pd.isnull(row[col])]\n        matching_predictions = [col for col in prediction_cols if col.replace('prediction', 'target') in non_null_targets]\n        \n        if not non_null_targets:\n            raise ValueError('No non-null target values found')\n        if row[non_null_targets].std(ddof=0) == 0 or row[matching_predictions].std(ddof=0) == 0:\n            raise ZeroDivisionError('Denominator is zero, unable to compute rank correlation.')\n        \n        return np.corrcoef(\n            row[matching_predictions].rank(method='average'), \n            row[non_null_targets].rank(method='average')\n        )[0, 1]\n    \n    daily_rank_corrs = merged_df.apply(_compute_rank_correlation, axis=1)\n    std_dev = daily_rank_corrs.std(ddof=0)\n    \n    if std_dev == 0:\n        raise ZeroDivisionError('Denominator is zero, unable to compute Sharpe ratio.')\n    \n    sharpe_ratio = daily_rank_corrs.mean() / std_dev\n    return float(sharpe_ratio)\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame) -> float:\n    assert all(solution.columns == submission.columns)\n    submission = submission.rename(columns={col: col.replace('target_', 'prediction_') for col in submission.columns})\n    solution = solution.replace(0, None)\n    return rank_correlation_sharpe_ratio(pd.concat([solution, submission], axis='columns'))\n\n# Test scoring\nscore_value = score(trainl[Features[1:]], df_wide[Features[1:]])\nprint(f\"SCORE: {score_value:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-10-04T08:16:54.182296Z","shell.execute_reply.started":"2025-10-04T08:16:44.423475Z","shell.execute_reply":"2025-10-04T08:16:54.181704Z"}},"outputs":[{"name":"stdout","text":"SCORE: 1.316328\n","output_type":"stream"}],"execution_count":4},{"id":"e9964e11-9bee-4816-9605-2995e455e6ee","cell_type":"code","source":"import polars as pl\n\ndef predict(\n    test: pl.DataFrame,\n    lag1: pl.DataFrame, \n    lag2: pl.DataFrame,\n    lag3: pl.DataFrame,\n    lag4: pl.DataFrame,\n) -> pl.DataFrame:\n    \"\"\"\n    Predicts target values using lag features.\n    This is your working version from the notebook.\n    \"\"\"\n    # Convert to pandas\n    test_pd = test.to_pandas()\n    lag1_pd = lag1.to_pandas()\n    lag2_pd = lag2.to_pandas()\n    lag3_pd = lag3.to_pandas()\n    lag4_pd = lag4.to_pandas()\n    \n    # Combine lag features\n    X_pred = pd.concat([\n        test_pd[[\"date_id\"]],\n        lag1_pd[target_lag_1],\n        lag2_pd[target_lag_2],\n        lag3_pd[target_lag_3],\n        lag4_pd[target_lag_4],\n    ], axis=1)\n    \n    # If no rows, return all zeros\n    if len(X_pred) == 0:\n        return pl.DataFrame(0, schema=[(f\"target_{i}\", pl.Float64) for i in range(424)])\n    \n    # Fill nulls with 0\n    X_pred = X_pred.fillna(0)\n    \n    # Prepare features for prediction\n    n_targets = 424\n    n_rows = X_pred.shape[0]\n    \n    # Create features for all targets\n    features_array = np.tile(X_pred[Features[1:]].values, (n_targets, 1))\n    target_ids = np.repeat(np.arange(n_targets), n_rows)\n    \n    # Create prediction DataFrame\n    X_pred2 = pd.DataFrame({\n        \"date_id\": np.tile(X_pred[\"date_id\"].values, n_targets),\n        **{feat: features_array[:, i] for i, feat in enumerate(Features[1:])},\n        \"target_id\": target_ids,\n        \"row\": np.tile(np.arange(n_rows), n_targets)\n    })\n    \n    # Make predictions\n    preds = ensemble_predict(Models, X_pred2[Features2])\n    X_pred2 = X_pred2.assign(preds=preds)\n    \n    # Pivot to wide format\n    df_wide = (\n        X_pred2.groupby([\"target_id\", \"row\"])\n        .agg({\"preds\": \"first\"})\n        .reset_index()\n        .pivot(index=\"row\", columns=\"target_id\", values=\"preds\")\n        .sort_index()\n    )\n    \n    # Ensure correct column order\n    df_wide = df_wide.reindex(columns=range(424), fill_value=0)\n    df_wide.columns = [f\"target_{i}\" for i in range(424)]\n    \n    # Return last row as predictions\n    result_df = df_wide.tail(1)\n    return pl.from_pandas(result_df.reset_index(drop=True))\n\n# Test the prediction function\ndef test_prediction():\n    \"\"\"Test the prediction function\"\"\"\n    sample_test = pl.from_pandas(trainl[Features].iloc[:5])\n    sample_lag1 = pl.from_pandas(trainl[target_lag_1].iloc[:5])\n    sample_lag2 = pl.from_pandas(trainl[target_lag_2].iloc[:5])\n    sample_lag3 = pl.from_pandas(trainl[target_lag_3].iloc[:5])\n    sample_lag4 = pl.from_pandas(trainl[target_lag_4].iloc[:5])\n    \n    result = predict(sample_test, sample_lag1, sample_lag2, sample_lag3, sample_lag4)\n    print(f\"Prediction result shape: {result.shape}\")\n    return result\n\ntest_result = test_prediction()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:19:06.904454Z","iopub.execute_input":"2025-10-04T08:19:06.904762Z","iopub.status.idle":"2025-10-04T08:19:07.408588Z","shell.execute_reply.started":"2025-10-04T08:19:06.904739Z","shell.execute_reply":"2025-10-04T08:19:07.408014Z"}},"outputs":[{"name":"stdout","text":"Prediction result shape: (1, 424)\n","output_type":"stream"}],"execution_count":6},{"id":"67b4c687-d8eb-494b-a56e-4c57c3218535","cell_type":"code","source":"import joblib\n\ndef save_models():\n    \"\"\"Save the trained models\"\"\"\n    joblib.dump(Models, '/kaggle/working/models_list.joblib')\n    print(f\"Saved {len(Models)} models\")\n\ndef load_models():\n    \"\"\"Load the trained models\"\"\"\n    Models = joblib.load('/kaggle/working/models_list.joblib')\n    print(f\"Loaded {len(Models)} models\")\n    return Models\n\n# Save models\nsave_models()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:19:44.417994Z","iopub.execute_input":"2025-10-04T08:19:44.418486Z","iopub.status.idle":"2025-10-04T08:19:44.687798Z","shell.execute_reply.started":"2025-10-04T08:19:44.418462Z","shell.execute_reply":"2025-10-04T08:19:44.687134Z"}},"outputs":[{"name":"stdout","text":"Saved 3 models\n","output_type":"stream"}],"execution_count":7},{"id":"48959ca6","cell_type":"code","source":"# submission through the API\nimport os\nimport kaggle_evaluation.mitsui_inference_server\n\ninference_server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print('there')\n    inference_server.serve()\nelse:\n    print('here')\n    inference_server.run_local_gateway(('/kaggle/input/mitsui-commodity-prediction-challenge/',))","metadata":{"papermill":{"duration":49.005971,"end_time":"2025-09-08T12:22:13.642777","exception":false,"start_time":"2025-09-08T12:21:24.636806","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:19:49.227234Z","iopub.execute_input":"2025-10-04T08:19:49.227767Z","iopub.status.idle":"2025-10-04T08:20:35.215333Z","shell.execute_reply.started":"2025-10-04T08:19:49.227736Z","shell.execute_reply":"2025-10-04T08:20:35.214518Z"}},"outputs":[{"name":"stdout","text":"here\n","output_type":"stream"}],"execution_count":8},{"id":"e9a1959c","cell_type":"code","source":"display(pl.read_parquet('/kaggle/working/submission.parquet'))","metadata":{"papermill":{"duration":0.141377,"end_time":"2025-09-08T12:22:13.791255","exception":false,"start_time":"2025-09-08T12:22:13.649878","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:21:15.701018Z","iopub.execute_input":"2025-10-04T08:21:15.701799Z","iopub.status.idle":"2025-10-04T08:21:15.747054Z","shell.execute_reply.started":"2025-10-04T08:21:15.701774Z","shell.execute_reply":"2025-10-04T08:21:15.746396Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"shape: (134, 425)\n┌─────────┬───────────┬───────────┬───────────┬───┬────────────┬───────────┬───────────┬───────────┐\n│ date_id ┆ target_0  ┆ target_1  ┆ target_2  ┆ … ┆ target_420 ┆ target_42 ┆ target_42 ┆ target_42 │\n│ ---     ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---        ┆ 1         ┆ 2         ┆ 3         │\n│ i64     ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64        ┆ ---       ┆ ---       ┆ ---       │\n│         ┆           ┆           ┆           ┆   ┆            ┆ f64       ┆ f64       ┆ f64       │\n╞═════════╪═══════════╪═══════════╪═══════════╪═══╪════════════╪═══════════╪═══════════╪═══════════╡\n│ 1827    ┆ -0.000471 ┆ -0.000476 ┆ -0.000476 ┆ … ┆ 0.000852   ┆ 0.001328  ┆ 0.001897  ┆ 0.001562  │\n│ 1828    ┆ -0.000471 ┆ -0.000476 ┆ -0.000476 ┆ … ┆ 0.000852   ┆ 0.001328  ┆ 0.001897  ┆ 0.001562  │\n│ 1829    ┆ -0.00053  ┆ -0.000535 ┆ -0.000535 ┆ … ┆ 0.000756   ┆ 0.001119  ┆ 0.001407  ┆ 0.001222  │\n│ 1830    ┆ -0.000476 ┆ -0.000481 ┆ -0.000481 ┆ … ┆ 0.000642   ┆ 0.001024  ┆ 0.001662  ┆ 0.001468  │\n│ 1831    ┆ 0.000032  ┆ 0.000027  ┆ 0.000027  ┆ … ┆ 0.000455   ┆ 0.000466  ┆ 0.001116  ┆ 0.000277  │\n│ …       ┆ …         ┆ …         ┆ …         ┆ … ┆ …          ┆ …         ┆ …         ┆ …         │\n│ 1956    ┆ -0.000294 ┆ -0.000294 ┆ -0.000294 ┆ … ┆ -0.000568  ┆ -0.000151 ┆ -0.000669 ┆ -0.006195 │\n│ 1957    ┆ -0.000138 ┆ -0.000143 ┆ -0.000143 ┆ … ┆ 0.000343   ┆ 0.000769  ┆ 0.000844  ┆ -0.003759 │\n│ 1958    ┆ 0.000171  ┆ 0.000166  ┆ 0.000166  ┆ … ┆ 0.000808   ┆ 0.001136  ┆ 0.00125   ┆ -0.001071 │\n│ 1959    ┆ 0.000655  ┆ 0.000655  ┆ 0.000655  ┆ … ┆ -0.000567  ┆ -0.000639 ┆ -0.000874 ┆ -0.001153 │\n│ 1960    ┆ 0.000241  ┆ 0.000236  ┆ 0.000236  ┆ … ┆ 0.001612   ┆ 0.001963  ┆ 0.003032  ┆ 0.0022    │\n└─────────┴───────────┴───────────┴───────────┴───┴────────────┴───────────┴───────────┴───────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (134, 425)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>target_0</th><th>target_1</th><th>target_2</th><th>target_3</th><th>target_4</th><th>target_5</th><th>target_6</th><th>target_7</th><th>target_8</th><th>target_9</th><th>target_10</th><th>target_11</th><th>target_12</th><th>target_13</th><th>target_14</th><th>target_15</th><th>target_16</th><th>target_17</th><th>target_18</th><th>target_19</th><th>target_20</th><th>target_21</th><th>target_22</th><th>target_23</th><th>target_24</th><th>target_25</th><th>target_26</th><th>target_27</th><th>target_28</th><th>target_29</th><th>target_30</th><th>target_31</th><th>target_32</th><th>target_33</th><th>target_34</th><th>target_35</th><th>&hellip;</th><th>target_387</th><th>target_388</th><th>target_389</th><th>target_390</th><th>target_391</th><th>target_392</th><th>target_393</th><th>target_394</th><th>target_395</th><th>target_396</th><th>target_397</th><th>target_398</th><th>target_399</th><th>target_400</th><th>target_401</th><th>target_402</th><th>target_403</th><th>target_404</th><th>target_405</th><th>target_406</th><th>target_407</th><th>target_408</th><th>target_409</th><th>target_410</th><th>target_411</th><th>target_412</th><th>target_413</th><th>target_414</th><th>target_415</th><th>target_416</th><th>target_417</th><th>target_418</th><th>target_419</th><th>target_420</th><th>target_421</th><th>target_422</th><th>target_423</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1827</td><td>-0.000471</td><td>-0.000476</td><td>-0.000476</td><td>-0.000477</td><td>-0.000477</td><td>-0.000479</td><td>-0.000479</td><td>-0.000473</td><td>-0.000473</td><td>-0.000357</td><td>-0.00035</td><td>-0.000324</td><td>-0.000324</td><td>-0.000324</td><td>-0.000324</td><td>-0.000381</td><td>-0.000381</td><td>-0.000431</td><td>-0.000431</td><td>-0.000433</td><td>-0.000434</td><td>-0.000451</td><td>-0.00044</td><td>-0.000454</td><td>-0.000454</td><td>-0.000468</td><td>-0.000452</td><td>-0.000449</td><td>-0.000462</td><td>-0.000605</td><td>-0.000673</td><td>-0.000673</td><td>-0.000673</td><td>-0.000632</td><td>-0.000632</td><td>-0.000593</td><td>&hellip;</td><td>-0.000826</td><td>-0.000861</td><td>-0.000541</td><td>-0.000497</td><td>-0.00093</td><td>-0.00095</td><td>0.000239</td><td>-0.001212</td><td>-0.001614</td><td>-0.001569</td><td>-0.001341</td><td>-0.001352</td><td>-0.001373</td><td>-0.001356</td><td>-0.001386</td><td>-0.001434</td><td>-0.001619</td><td>0.001855</td><td>-0.000247</td><td>-0.000393</td><td>-0.000393</td><td>-0.00024</td><td>-0.000254</td><td>-0.000237</td><td>-0.000798</td><td>-0.000003</td><td>-0.000872</td><td>0.000785</td><td>0.000325</td><td>0.000213</td><td>0.000455</td><td>0.000573</td><td>0.000564</td><td>0.000852</td><td>0.001328</td><td>0.001897</td><td>0.001562</td></tr><tr><td>1828</td><td>-0.000471</td><td>-0.000476</td><td>-0.000476</td><td>-0.000477</td><td>-0.000477</td><td>-0.000479</td><td>-0.000479</td><td>-0.000473</td><td>-0.000473</td><td>-0.000357</td><td>-0.00035</td><td>-0.000324</td><td>-0.000324</td><td>-0.000324</td><td>-0.000324</td><td>-0.000381</td><td>-0.000381</td><td>-0.000431</td><td>-0.000431</td><td>-0.000433</td><td>-0.000434</td><td>-0.000451</td><td>-0.00044</td><td>-0.000454</td><td>-0.000454</td><td>-0.000468</td><td>-0.000452</td><td>-0.000449</td><td>-0.000462</td><td>-0.000605</td><td>-0.000673</td><td>-0.000673</td><td>-0.000673</td><td>-0.000632</td><td>-0.000632</td><td>-0.000593</td><td>&hellip;</td><td>-0.000826</td><td>-0.000861</td><td>-0.000541</td><td>-0.000497</td><td>-0.00093</td><td>-0.00095</td><td>0.000239</td><td>-0.001212</td><td>-0.001614</td><td>-0.001569</td><td>-0.001341</td><td>-0.001352</td><td>-0.001373</td><td>-0.001356</td><td>-0.001386</td><td>-0.001434</td><td>-0.001619</td><td>0.001855</td><td>-0.000247</td><td>-0.000393</td><td>-0.000393</td><td>-0.00024</td><td>-0.000254</td><td>-0.000237</td><td>-0.000798</td><td>-0.000003</td><td>-0.000872</td><td>0.000785</td><td>0.000325</td><td>0.000213</td><td>0.000455</td><td>0.000573</td><td>0.000564</td><td>0.000852</td><td>0.001328</td><td>0.001897</td><td>0.001562</td></tr><tr><td>1829</td><td>-0.00053</td><td>-0.000535</td><td>-0.000535</td><td>-0.000536</td><td>-0.000536</td><td>-0.000538</td><td>-0.000538</td><td>-0.000532</td><td>-0.000532</td><td>-0.000413</td><td>-0.000406</td><td>-0.00038</td><td>-0.00038</td><td>-0.00038</td><td>-0.00038</td><td>-0.000431</td><td>-0.000431</td><td>-0.000478</td><td>-0.000478</td><td>-0.000481</td><td>-0.000482</td><td>-0.000504</td><td>-0.000492</td><td>-0.000506</td><td>-0.000506</td><td>-0.00052</td><td>-0.000504</td><td>-0.000495</td><td>-0.000508</td><td>-0.000652</td><td>-0.000722</td><td>-0.000722</td><td>-0.000722</td><td>-0.000674</td><td>-0.000674</td><td>-0.000636</td><td>&hellip;</td><td>-0.000896</td><td>-0.000931</td><td>-0.000611</td><td>-0.000566</td><td>-0.000999</td><td>-0.00102</td><td>0.000169</td><td>-0.001282</td><td>-0.001684</td><td>-0.00164</td><td>-0.001408</td><td>-0.001419</td><td>-0.00144</td><td>-0.001423</td><td>-0.001437</td><td>-0.001477</td><td>-0.001661</td><td>0.001818</td><td>-0.000289</td><td>-0.000426</td><td>-0.000426</td><td>-0.00029</td><td>-0.000303</td><td>-0.000286</td><td>-0.000836</td><td>-0.000041</td><td>-0.00091</td><td>0.000716</td><td>0.000256</td><td>0.00012</td><td>0.000363</td><td>0.000477</td><td>0.000467</td><td>0.000756</td><td>0.001119</td><td>0.001407</td><td>0.001222</td></tr><tr><td>1830</td><td>-0.000476</td><td>-0.000481</td><td>-0.000481</td><td>-0.000482</td><td>-0.000482</td><td>-0.00049</td><td>-0.00049</td><td>-0.000484</td><td>-0.000484</td><td>-0.000376</td><td>-0.000368</td><td>-0.000373</td><td>-0.000373</td><td>-0.000373</td><td>-0.000373</td><td>-0.000393</td><td>-0.000393</td><td>-0.000691</td><td>-0.000691</td><td>-0.000558</td><td>-0.000559</td><td>-0.000553</td><td>-0.000548</td><td>-0.000488</td><td>-0.000439</td><td>-0.000489</td><td>-0.00047</td><td>-0.000462</td><td>-0.000476</td><td>-0.000731</td><td>-0.000773</td><td>-0.000773</td><td>-0.000773</td><td>-0.000501</td><td>-0.000501</td><td>-0.00049</td><td>&hellip;</td><td>-0.000531</td><td>-0.000602</td><td>-0.000282</td><td>-0.00023</td><td>-0.000613</td><td>-0.000638</td><td>0.00054</td><td>-0.000877</td><td>-0.001273</td><td>-0.001211</td><td>-0.000948</td><td>-0.000948</td><td>-0.000962</td><td>-0.000945</td><td>-0.00092</td><td>-0.000917</td><td>-0.001482</td><td>0.002278</td><td>0.000142</td><td>0.00001</td><td>0.00001</td><td>0.00018</td><td>0.000165</td><td>0.000159</td><td>-0.000305</td><td>0.000484</td><td>-0.000325</td><td>0.000511</td><td>0.000438</td><td>0.000173</td><td>0.000391</td><td>0.000529</td><td>0.00053</td><td>0.000642</td><td>0.001024</td><td>0.001662</td><td>0.001468</td></tr><tr><td>1831</td><td>0.000032</td><td>0.000027</td><td>0.000027</td><td>0.000027</td><td>0.000027</td><td>0.000018</td><td>0.000018</td><td>0.000019</td><td>0.000019</td><td>0.000064</td><td>0.000059</td><td>0.000069</td><td>0.000069</td><td>0.000069</td><td>0.000069</td><td>0.00007</td><td>0.00007</td><td>0.000026</td><td>0.000026</td><td>0.000085</td><td>0.000084</td><td>0.00005</td><td>0.000061</td><td>0.000046</td><td>0.000098</td><td>0.000047</td><td>0.000066</td><td>0.000108</td><td>0.000095</td><td>-0.000191</td><td>-0.000258</td><td>-0.000247</td><td>-0.000241</td><td>-0.000025</td><td>-0.000025</td><td>0.000003</td><td>&hellip;</td><td>-0.00036</td><td>-0.000448</td><td>-0.000128</td><td>-0.000113</td><td>-0.000418</td><td>-0.000434</td><td>-0.001345</td><td>-0.00096</td><td>-0.001062</td><td>-0.000995</td><td>-0.000718</td><td>-0.000724</td><td>-0.000725</td><td>-0.000708</td><td>-0.000692</td><td>-0.00058</td><td>-0.001297</td><td>0.002094</td><td>0.000151</td><td>0.000149</td><td>0.000149</td><td>0.000501</td><td>0.000465</td><td>0.000278</td><td>0.000145</td><td>0.000446</td><td>0.000117</td><td>0.000341</td><td>0.000233</td><td>-0.000014</td><td>0.000236</td><td>0.000173</td><td>0.000174</td><td>0.000455</td><td>0.000466</td><td>0.001116</td><td>0.000277</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1956</td><td>-0.000294</td><td>-0.000294</td><td>-0.000294</td><td>-0.000294</td><td>-0.000294</td><td>-0.000301</td><td>-0.000301</td><td>-0.000299</td><td>-0.000299</td><td>-0.000293</td><td>-0.000286</td><td>-0.00025</td><td>-0.00025</td><td>-0.00025</td><td>-0.00025</td><td>-0.000346</td><td>-0.000346</td><td>-0.000371</td><td>-0.000371</td><td>-0.000373</td><td>-0.000374</td><td>-0.000368</td><td>-0.000362</td><td>-0.000383</td><td>-0.000383</td><td>-0.000423</td><td>-0.000427</td><td>-0.000482</td><td>-0.000495</td><td>-0.000671</td><td>-0.00073</td><td>-0.00073</td><td>-0.000731</td><td>-0.000731</td><td>-0.000731</td><td>-0.000695</td><td>&hellip;</td><td>-0.000058</td><td>-0.000098</td><td>-0.000078</td><td>-0.000538</td><td>-0.000545</td><td>-0.000577</td><td>-0.00029</td><td>-0.000239</td><td>-0.000466</td><td>-0.000496</td><td>-0.000346</td><td>-0.00034</td><td>-0.000317</td><td>-0.000317</td><td>-0.000327</td><td>-0.000115</td><td>0.000059</td><td>0.001538</td><td>0.000389</td><td>0.000249</td><td>0.000238</td><td>0.000394</td><td>0.000362</td><td>0.000107</td><td>-0.000617</td><td>0.00054</td><td>-0.000243</td><td>0.000151</td><td>0.000038</td><td>-0.000464</td><td>-0.000497</td><td>-0.000496</td><td>-0.000595</td><td>-0.000568</td><td>-0.000151</td><td>-0.000669</td><td>-0.006195</td></tr><tr><td>1957</td><td>-0.000138</td><td>-0.000143</td><td>-0.000143</td><td>-0.000143</td><td>-0.000143</td><td>-0.000145</td><td>-0.000145</td><td>-0.00014</td><td>-0.00014</td><td>-0.000008</td><td>-5.2223e-7</td><td>0.000036</td><td>0.000036</td><td>0.000036</td><td>0.000036</td><td>-0.000015</td><td>-0.000015</td><td>-0.000043</td><td>-0.000043</td><td>-0.000046</td><td>-0.000047</td><td>-0.000105</td><td>-0.000087</td><td>-0.000102</td><td>-0.000102</td><td>-0.000136</td><td>-0.000138</td><td>-0.00014</td><td>-0.000153</td><td>-0.000548</td><td>-0.000634</td><td>-0.000634</td><td>-0.000634</td><td>-0.000357</td><td>-0.000357</td><td>-0.000389</td><td>&hellip;</td><td>0.000103</td><td>0.000054</td><td>0.000166</td><td>0.000183</td><td>-0.00035</td><td>-0.000443</td><td>-0.000049</td><td>-0.0001</td><td>-0.000481</td><td>-0.000677</td><td>-0.000346</td><td>-0.000361</td><td>-0.000428</td><td>-0.000364</td><td>-0.000354</td><td>-0.000475</td><td>-0.000457</td><td>0.001138</td><td>0.000292</td><td>0.000139</td><td>0.000139</td><td>0.000328</td><td>0.000314</td><td>0.000274</td><td>-0.000631</td><td>0.00083</td><td>-0.00051</td><td>0.001091</td><td>0.000608</td><td>0.000499</td><td>0.000595</td><td>0.00031</td><td>0.000277</td><td>0.000343</td><td>0.000769</td><td>0.000844</td><td>-0.003759</td></tr><tr><td>1958</td><td>0.000171</td><td>0.000166</td><td>0.000166</td><td>0.000157</td><td>0.000157</td><td>0.000166</td><td>0.000166</td><td>0.000172</td><td>0.000172</td><td>0.000295</td><td>0.000303</td><td>0.000363</td><td>0.000363</td><td>0.00038</td><td>0.00038</td><td>0.000338</td><td>0.000338</td><td>0.000381</td><td>0.000381</td><td>0.000384</td><td>0.000383</td><td>0.000307</td><td>0.000287</td><td>0.000257</td><td>0.000257</td><td>0.000233</td><td>0.000232</td><td>0.000294</td><td>0.000281</td><td>-0.000029</td><td>-0.000103</td><td>-0.000099</td><td>-0.000099</td><td>-0.000146</td><td>-0.000146</td><td>-0.000098</td><td>&hellip;</td><td>0.000052</td><td>0.00006</td><td>0.00011</td><td>-0.000159</td><td>-0.000491</td><td>-0.00049</td><td>-0.000267</td><td>-0.000199</td><td>-0.000563</td><td>-0.000748</td><td>-0.000454</td><td>-0.000465</td><td>-0.000638</td><td>-0.000615</td><td>-0.000607</td><td>-0.000693</td><td>-0.000879</td><td>0.000373</td><td>-0.000305</td><td>-0.000347</td><td>-0.000347</td><td>-0.000159</td><td>-0.00015</td><td>-0.000131</td><td>-0.000357</td><td>0.000362</td><td>-0.000843</td><td>0.000959</td><td>0.000428</td><td>0.000452</td><td>0.000816</td><td>0.000442</td><td>0.000418</td><td>0.000808</td><td>0.001136</td><td>0.00125</td><td>-0.001071</td></tr><tr><td>1959</td><td>0.000655</td><td>0.000655</td><td>0.000655</td><td>0.000655</td><td>0.000655</td><td>0.000643</td><td>0.000643</td><td>0.000643</td><td>0.000643</td><td>0.000602</td><td>0.000602</td><td>0.000648</td><td>0.000648</td><td>0.000668</td><td>0.000668</td><td>0.000643</td><td>0.000643</td><td>0.001973</td><td>0.001973</td><td>0.001973</td><td>0.00197</td><td>0.001945</td><td>0.002137</td><td>0.002121</td><td>0.002131</td><td>0.001396</td><td>0.00135</td><td>0.001428</td><td>0.001417</td><td>0.000233</td><td>0.00012</td><td>0.000129</td><td>0.000129</td><td>0.000254</td><td>0.000254</td><td>0.000315</td><td>&hellip;</td><td>-0.000426</td><td>-0.000384</td><td>-0.000361</td><td>-0.000703</td><td>-0.000875</td><td>-0.001016</td><td>-0.001035</td><td>-0.000924</td><td>-0.001117</td><td>-0.001269</td><td>-0.000938</td><td>-0.000979</td><td>-0.001073</td><td>-0.00105</td><td>-0.00103</td><td>-0.000991</td><td>-0.001083</td><td>0.000236</td><td>-0.000638</td><td>-0.000641</td><td>-0.000652</td><td>-0.000487</td><td>-0.000533</td><td>-0.000498</td><td>-0.000765</td><td>-0.000386</td><td>-0.000717</td><td>-0.000287</td><td>-0.000399</td><td>-0.000646</td><td>-0.0002</td><td>-0.000708</td><td>-0.000712</td><td>-0.000567</td><td>-0.000639</td><td>-0.000874</td><td>-0.001153</td></tr><tr><td>1960</td><td>0.000241</td><td>0.000236</td><td>0.000236</td><td>0.000247</td><td>0.000247</td><td>0.000245</td><td>0.000245</td><td>0.000245</td><td>0.000245</td><td>0.000262</td><td>0.000263</td><td>0.000279</td><td>0.000279</td><td>0.000279</td><td>0.000279</td><td>0.000246</td><td>0.000246</td><td>0.000447</td><td>0.000447</td><td>0.000445</td><td>0.000455</td><td>0.000424</td><td>0.000436</td><td>0.000426</td><td>0.000436</td><td>0.000221</td><td>0.000213</td><td>0.000244</td><td>0.000238</td><td>0.00016</td><td>0.00014</td><td>0.000163</td><td>0.000167</td><td>0.000153</td><td>0.000153</td><td>0.000215</td><td>&hellip;</td><td>-0.000237</td><td>-0.00024</td><td>-0.000153</td><td>-0.000414</td><td>-0.000517</td><td>-0.000662</td><td>-0.000927</td><td>-0.001181</td><td>-0.000874</td><td>-0.000977</td><td>-0.00083</td><td>-0.000855</td><td>-0.000732</td><td>-0.000732</td><td>-0.000747</td><td>-0.000695</td><td>-0.000702</td><td>0.001262</td><td>0.000007</td><td>-0.000236</td><td>-0.000236</td><td>-0.000023</td><td>-0.000022</td><td>0.00002</td><td>-0.000251</td><td>0.00033</td><td>-0.000535</td><td>0.001172</td><td>0.000938</td><td>0.000789</td><td>0.001162</td><td>0.001297</td><td>0.001358</td><td>0.001612</td><td>0.001963</td><td>0.003032</td><td>0.0022</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":9},{"id":"92645eab","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.00639,"end_time":"2025-09-08T12:22:13.818388","exception":false,"start_time":"2025-09-08T12:22:13.811998","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}